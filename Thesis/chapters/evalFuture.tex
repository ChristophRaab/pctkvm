\chapter{Evaluation and Future Work}\label{EvalF}	
This chapter is devoted to critically review the characteristics of the study and consider future work.\\
The study classifiers used in this thesis are primarily based on the kernel from \eqref{EqRBFAKernel}.
According to the corresponding articles, the \acs{TCA} \acs{JDA} approaches are using the standard Gaussian kernel from \eqref{EqRBFOriginalKernel}.\cite{Long.}\cite{Pan.2011}
In there, we can naturally observe different performances, may better or worse.
For more insights, whether the transfer learning performance is rather critical to the suggested kernel type, it would be useful to repeat the study with the suggested kernels from the corresponding articles.
Furthermore, it should be compared to this study to determine the differences.
Additionally, the ability to transfer knowledge of the \acs{SVM} is comparable to the transfer learning solutions with the use of the new kernel, according to Friedman test from table \ref{TableFriedman}.
In a new study, we can verify, whether the \acs{SVM} works comparably good with the standard Gaussian kernel or not.
With that it can be seen if the \acs{SVM} is, in fact, better for the transfer learning settings as the \acs{PCVM}.\\
Another topic which is not included in this work, is the use of other datasets, to give a more general evidence, whether a transfer learning solution is better or not.
For example the 20-Newsgroup\footnote{\url{http://qwone.com/~jason/20Newsgroups/}} dataset, as a text-based dataset, as suggested in \cite{Pan.2010}.
The reason why we did not include this dataset is that the \acs{GFK} algorithm needs to calculate the null space of the dataset.
This task is main-memory consuming and can not be done by the for the study available computers, which results finally in an 'out of memory' error from MatLab.
We decided for the comparison with the \acs{PCTKVM} that it would be better to include more transfer learning solutions rather than datasets.\\
Other examples for image transfer learning datasets are \textit{USPS}\footnote{\url{http://www-i6.informatik.rwth-aachen.de/~keysers/usps.html}}, \textit{MNIST}\footnote{\url{http://yann.lecun.com/exdb/mnist/}}, \textit{COIL20}\footnote{\url{http://www.cs.columbia.edu/CAVE/software/softlib/coil-20.php}}.\\\cite{Long.}\cite{Long.2014}
These are not included in the study. Again for a more general assumption on the performance, it would be useful.
As well as including real-world scenarios like WiFi-localization or human activity classification as suggested in \cite[p. 32]{Weiss.2016}.\\
Furthermore, as described in \ref{EmSubSecMetricResult}, a more analytic way to determine the width of the Gaussian kernel should be found. Moreover, the current estimation should be modified in a future work to improve the performance and reduce the 'smoothness' a little bit.
A possible approach would be to incorporate another estimation for finding the theta for separated classes in the training set only. In this case, we could decide whether the theta over all sample is too large.\\
In the course of this study, we observed an overall good performance. However, we have been observed a mixed performance for  small multi-class image problems. Furthermore, we can also observe a drop of performance by the \textit{people vs. places} dataset, which is larger than the image sets.
In future work, the reason for the mixed performance and the performance dips could be determined.\\
\subsubsection{EM-Based Transfer Learning}
Another point worth mentioning is that the \acs{PCTKVM} is based on a non-probabilistic transfer learning method. 
However, the \acs{PCVM} are based on probabilistic assumptions, which is one of the achievements of Chen et al.
In future work, the issue could be addressed to find a probabilistic transfer learning method.
For example, as mentioned in section \ref{TlSubSecInduc}, the \acl{ITL} is related to self-thought learning.
With this, an idea would be to extend the current \acs{EM} algorithm of the \acs{PCVM} to learn the source and target domain simultaneously and transferring or modify the parameters from one into the other.
With that we could extend the \acs{PCVM} with transfer learning in a probabilistic manner.
A recent approach for 'self-paced' learning which uses a latent variable model and the \acs{EM} algorithm is a step in this direction and proposed in \cite{Kumar.2010}.
\subsubsection{Rotational Transfer Learning}
Inspired by the ideas of \acs{GFK} and \acs{TKL}, we could formulate another transfer learning approach.\\
We make the assumption that the marginal distributions are aligned with the use of the z-score.
With that the transfer learning problem would be reduced to aligning the conditional probabilistic distributions.
This idea is based on the \acl{SVD}, which can be used to decompose the rectangular source and target matrices with:\cite{Golub.1965} 
\begin{equation}
\mathbf{Z}= \mathbf{U}_\mathcal{Z}\boldsymbol{\Lambda}\mathbf{V}_\mathcal{Z} \land \mathbf{X} = \mathbf{U}_\mathcal{X}\boldsymbol{\Gamma}\mathbf{V}_\mathcal{X}
\end{equation}
Where $\mathbf{U}$ and $\mathbf{V}$ are rotation matrices and $\boldsymbol{\Lambda}$ and $\boldsymbol{\Gamma}$ are the singular values.\\
To do the transfer, one idea is to 'rotate'(approximate) the source data with the use of the rotations matrices of the target data with:
\begin{equation}
\mathbf{Z}= \mathbf{U}_\mathcal{X}\boldsymbol{\Lambda}\mathbf{V}_\mathcal{X}
\end{equation}   
Note that with this solution, the size of samples has to be aligned, which is currently solved with a Latent-Semantic-Analysis.
With that the training data could be extended with the structure information obtained from the target data.
The error of this approximation can be bounded by $E_{RT} =\Vert tr(\boldsymbol{\Lambda}) - tr(\boldsymbol{\Gamma})\Vert$.
A first test has been showed that the performance is outstanding for text sets, but has a worse performance on image datasets.
Note that because this idea is not finished yet, the approach is not in the study. 
One can find the first version of the \ac{RT-PCVM} in the source code of this thesis.
In a further work, the design of this approach could be finished.
\section{Extensions}\label{InSubSecFExt}
Although, future works have already been mentioned in the course of this chapter, we want to point out some further extensions, which can help to make the \acs{PCTKVM} applicable.
These extensions are coming mainly from \cite{Long.2015}.\\
The current algorithm can be extended to multiple source learning.
With that we can match the distribution difference of multiple source domains with the target domain.
This can be implemented by learning a $\mathbf{\Lambda}$ for every source/target combination and then using an existing multiple source learning, approach to predict the target based on multiple source domains.
For example, one could use the \ac{CP-MDA} algorithm from \cite{Chattopadhyay.2012} for it.\\
Another suggested extension is to adopt the algorithm to 'big data'.
Because, a kernel machine can have at least $\mathcal{O}(N^2)$ or even worse, they may not be applied well to huge datasets.
Here we can make use of the original Nyström method from \ref{InSecNysMeth} in combination with the Nyström kernel approximation from \ref{InSubSecNyKerneLearning}.\cite{Long.2015}\\
Consider a large test kernel matrix $\mathbf{K}_\mathcal{X} \in \mathbb{R}^{M\times M}$.
Then draw a sample from this kernel in an \acs{IID} manner to form a subset kernel $\mathbf{K}_\mathcal{X\hat{X}} \in \mathbb{R}^{M\times \hat{M}}$ with $\hat{M} \ll M$, which forms the subset $\mathcal{\hat{X}}$.
In the next step the eigensystem can be calculated through \eqref{EqEigsProb} and then the eigensystem of the target kernel can be extrapolated with:\cite{Long.2014}
\begin{equation}\label{EqLargeNsystromApprox}
\mathbf{U}_\mathcal{X}=\mathbf{K}_\mathcal{X\hat{X}}\mathbf{U}_\mathcal{\hat{X}}\mathbf{\Lambda}_\mathcal{X}^{-1}
\end{equation}
Furthermore, we can approximate the source kernel $\mathbf{K}_{Z}$ with the  cross-domain kernel $\mathbf{K}_{\mathcal{Z\hat{Z}}} \in \mathbb{R}^{N\times \hat{M}}$ and the subset kernel $\mathbf{K}_\mathcal{\hat{Z}}$.
The procedure is similar to \eqref{EqLargeNsystromApprox} or with the use of the kernels only, shown in \eqref{EqTrainTestApprox} or \eqref{EqNystKernelApprox}, by replacing the target kernel with $\mathbf{K}_{\hat{Z}}$.
\begin{equation}
\mathbf{K}_\mathcal{Z} \simeq	\mathbf{K}_{\mathcal{Z\hat{Z}}}\mathbf{K}_\mathcal{\hat{Z}}^{-1}\mathbf{K}_{\mathcal{\hat{Z}Z}}
\end{equation}
For being able to solve the quadratic programming problem from \eqref{EqTklQP}, we need to approximate the extrapolated eigensystem.
First, we extrapolate the eigensystem of the target kernel to the source subset via \eqref{EqExtraEigs}:
\begin{equation}
\expP{\mathbf{U}}_\mathcal{\hat{Z}}\simeq\mathbf{K}_{\mathcal{\hat{Z}X}}\mathbf{U}_{\mathcal{X}}\boldsymbol{\Lambda}_\mathcal{\hat{X}}^{-1}
\end{equation}
Second, the approximated eigensystem has to be extrapolated to the full source data:
\begin{equation}
\expP{\mathbf{U}}_\mathcal{Z} \simeq \mathbf{K}_{\mathcal{Z\hat{Z}}}\expP{\mathbf{U}}_\mathcal{\hat{Z}}\boldsymbol{\Lambda}_\mathcal{\hat{X}}^{-1}
\end{equation}
With this, we have the required information to solve the \acl{QP} and are making the \acs{TKL} algorithm and applicable for big data.\cite{Long.2015}\\
Because it is a Nyström approximation, the quality of the approximation depends a lot on the quality of the taken sample, as mentioned in \ref{InSubSecNyKerneLearning}.