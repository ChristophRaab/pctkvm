\chapter{Evaluation and Future Work}\label{EvalF}	
%TODO: Future Work and Eval: Generalisierte implementierung der PCVM
This chapter is devoted to critical review the characteristics of the study and future work.\\
The study classifier used in this thesis are primarily based on the kernel from \eqref{EqRBFAKernel}.
According to the corresponding articles, they using the standard Gaussian kernel from \eqref{EqRBFOriginalKernel}.\cite{Long.}\cite{Gong.}\cite{Pan.2011}
There we can naturally observe different performances, may better or worse.
For more insights, whether the transfer learning performance is rather critical to the suggested kernel type, it would be useful to repeat the study with the suggested kernels from the corresponding articles.
Furthermore, it should be compared to this study to determine the differences.
Additional, the ability to transfer knowledge of the \acs{SVM} is comparable to the transfer learning solutions with the use of the new kernel, according to Friedman test from table \ref{TableFriedman}.
In a new study, we can verify, whether the \acs{SVM} works comparable good with the standard Gaussian kernel or not.
With that, it can be seen if the \acs{SVM} is, in fact, better for the transfer learning settings as the \acs{PCVM}.\\
Another topic which is not included in this work is the use of more datasets, to give a more general evidence, whether a transfer learning solution is better or not.
For example the 20-Newsgroup\footnote{http://qwone.com/~jason/20Newsgroups/} dataset, as a text based dataset, as suggested in \cite{Pan.2010}.
The reason why we did not include this dataset is that the \acs{GFK} algorithm needs to calculate the null space of the dataset.
This task is a main-memory consuming and can not be done by the for the study available computers, which results finally in an 'out of memory' error from Matlab.
We decided for the comparison with the \acs{PCTKVM}, and it would be better to include more transfer learning solutions than datasets.\\
Other examples for image transfer learning datasets \textit{USPS}\footnote{http://www-i6.informatik.rwth-aachen.de/~keysers/usps.html}, \textit{MNIST}\footnote{http://yann.lecun.com/exdb/mnist/}, \textit{COIL20}\footnote{http://www.cs.columbia.edu/CAVE/software/softlib/coil-20.php}.\cite{Long.}\cite{Long.2014}
These are not included in the study. Again for a more general assumption on the performance, it would be useful.
As well as real-world scenarios like WiFi-localisation or human activity classification.\cite[p. 32]{Weiss.2016}
At least we tested our transfer learning solution with the myoelectric prosthesis as real-world scenario.\\
Furthermore, as described in \ref{EmSubSecMetricResult}, a more analytical way to determine the width of the Gaussian kernel should be found. Moreover, the current estimation should be modified in a future work to improve the performance and reduce the 'smoothness' of it.
A possible approach would be to incorporate another estimation for finding the theta between the separated classes in the training set and combining them with the overall theta. In this case, we could decide whether the theta over all sample is too large.\\
In the course of this study, we observed an overall good performance. However, the biggest problem is, that the performance drops dramatically for certain datasets. For the image datasets, we assume that the reason for this is only a few training examples per domain. However, we can also observe a drop of performance by the \textit{people vs places} dataset, which has more than 1000 samples.
In future work, the reason for the dips could be determined.\\
Another point worth mentioning is that the \acs{PCTKVM} is based on a non-probabilistic transfer learning method. 
However, the \acs{PCVM} are based on probabilistic assumptions.
In future work, the issue could be addressed to find a probabilistic transfer learning method.
For example, as mentioned in section \ref{TlSubSecInduc}, the \acl{ITL} is related to self-thought learning.
With this, an idea would be to extend the current \acs{EM} algorithm of the \acs{PCVM} to learn the source and target domain simultaneously and transferring or modify the parameters from one into the other.
With that, we could extend the \acs{PCVM} with a transfer learning in a probabilistic manner.
A recent approach for 'self-paced' learning which uses a latent variable model and the \acs{EM} algorithm is a step in this direction and proposed in \cite{Kumar.2010}.