\chapter{Details to Algorithms}\label{appac}
In this appendix, the detailed steps for the \acs{PCTKVM} algorithm are presented.
We omit it the steps, which are already described in \acs{PCVM} and \acs{TKL} for clarity.
The complete algorithm procedure is shown in algorithm \ref{PCTKVMComplete}.\\
We also have extended the algorithm with the faster approach, where the dissimilar matrix is reused over tasks.
Furthermore, we have extended the pseudo code with the option, which allows the user to decide, whether he wants to specify the $\theta$ or proceed with the estimation.
If one decides to choose a $\theta$ on his own, then the model has two parameters: The width of the Gaussian kernel and the $\zeta$ eigenspectrum dumping factor.
Moreover, \acs{PCTKVM} algorithm has following algorithm parameter:
The \acs{PCTKVM} supports the standard Gaussian kernel, the new Gaussian kernel from \eqref{EqRBFAKernel} and the Laplacian kernel. But we strongly recommend the use of the new Gaussian kernel.\\
Furthermore the number of iterations \textit{niter} for the \acs{PCVM} can be specified and the convergence criteria as \textit{threshold}.
Furthermore in algorithm \ref{PseudoCodePcvm} the pseudo code for the \acs{PCVM} and a line explanation can be found.

\begin{algorithm}[!]
	\caption{Probabilistic Classification Transfer Kernel Vector Machine }\label{PCTKVMComplete}
	\begin{algorithmic}[1]
		\Require Input Data $\mathbf{K} = [\mathbf{Z};\mathbf{X}]$ as $N_\mathcal{Z}$ sized training and $N_\mathcal{X}$ sized text set; $\mathbf{Y}$ as $N$ sized training label vector; kernel(-type) \textit{ker}; eigenspectrum dumping factor $\zeta$; $\theta$ as kernel paramater; \textit{niter} as maximal number of iterations; \textit{threshold} $\tau$ as convergence criteria; \textbf{InitVector} as $N_\mathcal{Z}$-sized initialization vector.
		\Ensure Weight vector $\mathbf{w}$; bias $b$, kernel parameter $\theta$; transfer kernel $\expP{\mathbf{K}}_\mathcal{A}$.
		\State $\mathbf{D}$ = calculate\_Dissimilarity\_Matrix($\mathbf{K}$);
		\If{$\theta == -1$}
		\State $\theta$ = theta\_Estimation($\mathbf{D}$);  \Comment{According to section \eqref{InSubSecTheta}}
		\EndIf
		\State [$\mathbf{K}_\mathcal{Z}$, $\mathbf{K}_\mathcal{X}$,$\mathbf{K}_\mathcal{ZX}$]=compute\_Gaussian\_Kernel($\mathbf{D}$,\textit{ker},$\theta$),
		\State [$\{\mathbf{\Lambda}_\mathcal{X}, \mathbf{U}_\mathcal{X}\}$]= eigenDecompose($\mathbf{K}_\mathcal{X}$)  \Comment{Eq. \eqref{EqEigsProb}}
		\State [$\expP{\mathbf{U}}_\mathcal{Z}$] =extrapolate\_Eigensystem($\mathbf{K}_\mathcal{ZX},\mathbf{U}_\mathcal{X},\boldsymbol{\Lambda}_\mathcal{X}^{-1}$) \Comment{Eq. \eqref{EqExtraEigs}}
		\State [$\boldsymbol{\Lambda}$]=quadratic\_Programming($\mathbf{Q},\mathbf{C},r$)  \Comment{Eq. \eqref{EqTklQP} and constrains \eqref{EqTklQPCons}}
		\State		[$\expP{\mathbf{K}}_\mathcal{A}$]=compute\_Extrapolated\_Kernel($\expP{\mathbf{U}}_\mathcal{Z},\boldsymbol{\Lambda},\mathbf{U}_\mathcal{X}$) \Comment{Eq. \eqref{EqTKLKernel}}
		\State	[$\mathbf{w},b$] = initialize(\textbf{initVector});
		\State \textbf{nonZero} = determine\_nonZero\_Vector($\mathbf{w}$); 
		\For{$i=1$ to \textit{niter}}
		\State $\mathbf{w}^{new}$ = weight\_Update($\expP{\mathbf{K}}_{\mathcal{Z}},\mathbf{w,Y,nonZero}$);   \Comment{Eq. \eqref{EqPcvmUpdateW}}
		\State $b^{new}$ = bias\_update($\expP{\mathbf{K}}_{\mathcal{Z}},b\mathbf{,Y,nonZero}$); \Comment{Eq. \eqref{EqPcvmUpdateb}}
		\State $\theta^{new}$ = parameter\_update($\expP{\mathbf{K}}_{\mathcal{Z}},\mathbf{Y},\mathit{ker},\theta,\mathbf{w}^{new},b^{new},\mathbf{nonZero}$); \Comment{Eq. \eqref{EqPcvmDevTheta}}
		\State $\mathbf{nonZero}^{new}$ = determine\_nonZero\_Vector($\mathbf{w}^{new}$);
		\If{max(abs($\mathbf{w}^{new}-\mathbf{w}$)) $<$ $\tau$}
		\State break;
		\Else
		\State continue;
		\EndIf
		\EndFor
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
	\caption{Probabilistic Classification Vector Machine}\label{PseudoCodePcvm}	
	\begin{algorithmic}[1]
		\Require $\mathbf{I}$ =\{\textbf{X,Y}\} = $\{(x_n,y_n)\}_{n=1}^{N}$ as $N_\mathcal{Z}$ sized training set;
		\textit{ker} as kernel type; The kernel parameter $\theta$; \textit{niter} as maximal number of iterations; \textit{threshold} $\tau$ as convergence criteria; \textbf{InitVector} as $N_\mathcal{Z}$-sized initialization vector.
		\Ensure Weight Vector $\mathbf{w}$, bias $b$ and the kernel parameter $\theta$.
		\State	[$\mathbf{w},b$] = initialize(\textbf{initVector});
		\State \textbf{nonZero} = determine\_nonZero\_Vector($\mathbf{w}$); 
		\For{$i=1$ to \textit{niter}}
		\State $\Phi$ = Calculate\_Kernel($\mathbf{X,Y}$,\textit{ker},$\theta$); 
		\State $\mathbf{w}^{new}$ = weight\_update($\mathbf{\Phi,w,Y,nonZero}$);  
		\State $b^{new}$ = bias\_update($\mathbf{\Phi},b,\mathbf{Y,nonZero}$);
		\State $\theta^{new}$ = parameter\_update($\mathbf{\Phi,X,Y},\mathit{ker},\theta,\mathbf{w}^{new},b^{new},\mathbf{nonZero}$);
		\State $\mathbf{nonZero}^{new}$ = determine\_nonZero\_Vector($\mathbf{w}^{new}$);
		\If{max(abs($\mathbf{w}^{new}-\mathbf{w}$)) $<$ $\tau$ }
		\State break;
		\Else
		\State continue;
		\EndIf
		\EndFor
	\end{algorithmic}
\end{algorithm}
\algcomment{Explanation of the Steps within the Algorithm.
	\begin{enumerate}[noitemsep]
		\item Line 1 and 2: (Random) initialization of the weight vector $\mathbf{w}$ with $N$-sized \textbf{initVector} and bias $b$. Determining non zero elements in $\mathbf{w}$ and save it as index to \textbf{nonZero}.
		\item Line 4: Calculate the kernel $\Phi$.
		\item Line 5 and 6: Update $\mathbf{w}$ and $b$ with \eqref{EqPcvmUpdateW} and \eqref{EqPcvmUpdateb} respectively.
		\item Line 7: Solve optimization problem based on \eqref{EqPcvmDevTheta}.
		\item Line 9-15: Determine the convergence by comparing the difference of $\mathbf{w}^{new}$ and $\mathbf{w}$ against the threshold. Break if convergence happens, otherwise continue.
\end{enumerate}}\\
