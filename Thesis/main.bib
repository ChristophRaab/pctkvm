
@inproceedings{Raina.,
	author = {Raina, Rajat and Ng, Andrew Y. and Koller, Daphne},
	title = {{Constructing informative priors using transfer learning}},
	pages = {713--720},
	editor = {Cohen, William and Moore, Andrew},
	booktitle = {the 23rd international conference},
	doi = {10.1145/1143844.1143934}
}


@article{QuineroCandela.,
	author = {Qui駉nero-Candela, Joaquin},
	title = {{Dataset Shift in Machine Learning (Neural Information Processing)}}
}


@inproceedings{Quattoni.,
	author = {Quattoni, Ariadna and Collins, Michael and Darrell, Trevor},
	title = {{Transfer learning for image classification with sparse prototype representations}},
	pages = {1--8},
	booktitle = {{2008 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}},
	doi = {10.1109/CVPR.2008.4587637}
}




@book{Hartmann.2015,
	abstract = {Dieses Buch enth{\"a}lt in einem Band den Mathematik-Stoff, der f{\"u}r das Informatik-Studium in anwendungsorientierten Bachelor-Studieng{\"a}ngen ben{\"o}tigt wird. Der Inhalt entspringt der langj{\"a}hrigen Lehrerfahrung des Autors. Das hei{\ss}t: - Sie finden immer wieder Anwendungen aus der Informatik. - Sie lernen nicht nur mathematische Methoden, es werden auch die Denkweisen der Mathematik vermittelt, die eine Grundlage zum Verst{\"a}ndnis der Informatik bilden. - Beweise werden dann gef{\"u}hrt, wenn Sie daraus etwas lernen k{\"o}nnen, nicht um des Beweisens willen. Mathematik ist f{\"u}r viele Studierende zun{\"a}chst ein notwendiges {\"U}bel. Das Buch zeigt durch ausf{\"u}hrliche Motivation, durch viele Beispiele, durch das st{\"a}ndige Aufzeigen von Querbez{\"u}gen zwischen Mathematik und Informatik, dass Mathematik nicht nur n{\"u}tzlich ist, sondern interessant sein kann und manchmal auch Spa{\ss} macht. Der Inhalt Mengen, Logik, Zahlentheorie, Algebraische Strukturen, Kryptographie, Vektorr{\"a}ume, Matrizen, Lineare Gleichungen und Abbildungen, Eigenwerte, Graphentheorie Folgen und Reihen, Stetige Funktionen, Differenzial- und Integralrechnung, Differenzialgleichungen, Numerik Wahrscheinlichkeitstheorie und Statistik Die Zielgruppen Studierende in allen informatiknahen Bachelor-Studieng{\"a}ngen, Praktiker im Selbststudium Der Autor Peter Hartmann ist Professor an der Hochschule Landshut in der Fakult{\"a}t Informatik. Der Schwerpunkt seiner Lehrt{\"a}tigkeit liegt in der Mathematikausbildung f{\"u}r Informatiker und Wirtschaftsinformatiker},
	author = {Hartmann, Peter},
	year = {2015},
	title = {{Mathematik f{\"u}r Informatiker: Ein praxisbezogenes Lehrbuch}},
	address = {Wiesbaden},
	edition = {6., {\"u}berarb. Aufl.},
	publisher = {{Springer Vieweg}},
	isbn = {978-3-658-03415-3},
	doi = {10.1007/978-3-658-03416-0}
}


@article{PangNingTan.,
	author = {{Pang-Ning Tan} and {Michael Steinbach} and {Vipin Kumar}},
	title = {{Introduction to Data Mining}}
}


@article{Pan.2010,
	author = {Pan, Sinno Jialin and Yang, Qiang},
	year = {2010},
	title = {{A Survey on Transfer Learning}},
	pages = {1345--1359},
	volume = {22},
	number = {10},
	issn = {1041-4347},
	journal = {{IEEE Transactions on Knowledge and Data Engineering}},
	doi = {10.1109/TKDE.2009.191}
}


@article{Pan.2011,
	abstract = {Domain adaptation allows knowledge from a source domain to be transferred to a different but related target domain. Intuitively, discovering a good feature representation across domains is crucial. In this paper, we first propose to find such a representation through a new learning method, transfer component analysis (TCA), for domain adaptation. TCA tries to learn some transfer components across domains in a reproducing kernel Hilbert space using maximum mean miscrepancy. In the subspace spanned by these transfer components, data properties are preserved and data distributions in different domains are close to each other. As a result, with the new representations in this subspace, we can apply standard machine learning methods to train classifiers or regression models in the source domain for use in the target domain. Furthermore, in order to uncover the knowledge hidden in the relations between the data labels from the source and target domains, we extend TCA in a semisupervised learning setting, which encodes label information into transfer components learning. We call this extension semisupervised TCA. The main contribution of our work is that we propose a novel dimensionality reduction framework for reducing the distance between domains in a latent space for domain adaptation. We propose both unsupervised and semisupervised feature extraction approaches, which can dramatically reduce the distance between domain distributions by projecting data onto the learned transfer components. Finally, our approach can handle large datasets and naturally lead to out-of-sample generalization. The effectiveness and efficiency of our approach are verified by experiments on five toy datasets and two real-world applications: cross-domain indoor WiFi localization and cross-domain text classification.},
	author = {Pan, Sinno Jialin and Tsang, Ivor W. and Kwok, James T. and Yang, Qiang},
	year = {2011},
	title = {{Domain adaptation via transfer component analysis}},
	keywords = {Algorithms;Artificial Intelligence;Automatic Data Processing/methods;Computer Simulation/standards;Neural Networks (Computer);Pattern Recognition, Automated/methods;Transfer (Psychology)},
	pages = {199--210},
	volume = {22},
	number = {2},
	issn = {1045-9227},
	journal = {{IEEE transactions on neural networks}},
	doi = {10.1109/TNN.2010.2091281}
}

@inproceedings{Nam.,
	author = {Nam, Jaechang and Kim, Sunghun},
	title = {{Heterogeneous defect prediction}},
	keywords = {Defect prediction;heterogeneous metrics;quality assurance},
	pages = {508--519},
	editor = {{Di Nitto}, Elisabetta and Harman, Mark and Heymans, Patrick},
	booktitle = {{the 2015 10th Joint Meeting}},
	doi = {10.1145/2786805.2786814}
}


@article{RajatRaina.,
	author = {{Rajat Raina} and {Alexis Battle} and {Honglak Lee} and {Benjamin Packer} and {Andrew Y. Ng}},
	title = {{Self-taught Learning Transfer Learning from Unlabeled Data 2009}}
}


@article{Mizuno.2011,
	abstract = {Formin homology proteins (formins) elongate actin filaments (F-actin) by continuously associating with filament tips, potentially harnessing actin-generated pushing forces. During this processive elongation, formins are predicted to rotate along the axis of the double helical F-actin structure (referred to here as helical rotation), although this has not yet been definitively shown. We demonstrated helical rotation of the formin mDia1 by single-molecule fluorescence polarization (FL(P)). FL(P) of labeled F-actin, both elongating and depolymerizing from immobilized mDia1, oscillated with a periodicity corresponding to that of the F-actin long-pitch helix, and this was not altered by actin-bound nucleotides or the actin-binding protein profilin. Thus, helical rotation is an intrinsic property of formins. To harness pushing forces from growing F-actin, formins must be anchored flexibly to cell structures.},
	author = {Mizuno, Hiroaki and Higashida, Chiharu and Yuan, Yunfeng and Ishizaki, Toshimasa and Narumiya, Shuh and Watanabe, Naoki},
	year = {2011},
	title = {{Rotational movement of the formin mDia1 along the double helical strand of an actin filament}},
	keywords = {Actin Cytoskeleton/chemistry/metabolism/ultrastructure;Actins/chemistry/metabolism;Adenosine Diphosphate/metabolism;Adenosine Triphosphate/metabolism;Animals;Carrier Proteins/chemistry/metabolism;Fluorescence Polarization;Mice;Models, Biological;Profilins/metabolism;Protein Binding;Protein Interaction Domains and Motifs;Protein Multimerization;Protein Structure, Secondary;Rabbits;Recombinant Fusion Proteins/chemistry/metabolism;Rotation},
	pages = {80--83},
	volume = {331},
	number = {6013},
	issn = {1095-9203},
	journal = {{Science (New York, N.Y.)}},
	doi = {10.1126/science.1197692}
}


@article{MinwooLee.,
	author = {{Minwoo Lee} and {Charles W. Anderson}},
	title = {{Robust Reinforcement Learning with Relevance 2016}}
}


@article{MehmetGonenAdamA.Margolin.,
	author = {{Mehmet Gonen, Adam A. Margolin}},
	title = {{Kernelized Bayesian Transfer Learning}},
	keywords = {Novel Machine Learning Algorithms}
}




@article{Ma.2017,
	author = {Ma, Yuxin and Xu, Jiayi and Wu, Xiangyang and Wang, Fei and Chen, Wei},
	year = {2017},
	title = {{A visual analytical approach for transfer learning in classification}},
	keywords = {Classification;transfer learning;Visual analytics;Visualization},
	pages = {54--69},
	volume = {390},
	issn = {00200255},
	journal = {{Information Sciences}},
	doi = {10.1016/j.ins.2016.03.021}
}


@article{Lu.2015,
	author = {Lu, Jie and Behbood, Vahid and Hao, Peng and Zuo, Hua and Xue, Shan and Zhang, Guangquan},
	year = {2015},
	title = {{Transfer learning using computational intelligence: A survey}},
	keywords = {Bayes;Computational intelligence;Fuzzy sets and systems;Genetic algorithm;Neural network;transfer learning},
	pages = {14--23},
	volume = {80},
	issn = {09507051},
	journal = {{Knowledge-Based Systems}},
	doi = {10.1016/j.knosys.2015.01.010}
}


@article{Long.2015,
	author = {Long, Mingsheng and Wang, Jianmin and Sun, Jiaguang and Yu, Philip S.},
	year = {2015},
	title = {{Domain Invariant Transfer Kernel Learning}},
	pages = {1519--1532},
	volume = {27},
	number = {6},
	issn = {1041-4347},
	journal = {{IEEE Transactions on Knowledge and Data Engineering}},
	doi = {10.1109/TKDE.2014.2373376}
}


@inproceedings{Long.,
	author = {Long, Mingsheng and Wang, Jianmin and Ding, Guiguang and Sun, Jiaguang and Yu, Philip S.},
	title = {{Transfer Feature Learning with Joint Distribution Adaptation}},
	keywords = {feature learning;joint distribution adaptation;transfer learning},
	pages = {2200--2207},
	booktitle = {{2013 IEEE International Conference on Computer Vision (ICCV)}},
	doi = {10.1109/ICCV.2013.274}
}


@article{Long.2014,
	author = {Long, Mingsheng and Wang, Jianmin and Ding, Guiguang and Pan, Sinno Jialin and Yu, Philip S.},
	year = {2014},
	title = {{Adaptation Regularization: A General Framework for Transfer Learning}},
	pages = {1076--1089},
	volume = {26},
	number = {5},
	issn = {1041-4347},
	journal = {{IEEE Transactions on Knowledge and Data Engineering}},
	doi = {10.1109/TKDE.2013.111}
}


@book{LixinDuanDongXuShihFuChang.2012,
	author = {{Lixin Duan, Dong Xu, Shih-Fu Chang}},
	year = {2012},
	title = {{Exploiting Web Images for Event Recognition in Consumer Videos: A Multiple Source Domain Adaptation Approach: 16 - 21 June 2012, Providence, RI, USA}},
	url = {http://ieeexplore.ieee.org/servlet/opac?punumber=6235193},
	keywords = {Kongress;Maschinelles Sehen;Mustererkennung},
	address = {Piscataway, NJ},
	publisher = {IEEE},
	isbn = {9781467312288},
	institution = {{Institute of Electrical and Electronics Engineers} and {Computer Society} and {IEEE Conference on Computer Vision and Pattern Recognition} and CVPR and {IEEE Computer Society Conference on Computer Vision and Pattern Recognition}}
}


@article{MinwooLee.b,
	author = {{Minwoo Lee} and {Charles W. Anderson}},
	title = {{Robust Reinforcement Learning with Relevance Vector Machines}}
}


@article{Read.2011,
	author = {Read, Jesse and Pfahringer, Bernhard and Holmes, Geoff and Frank, Eibe},
	year = {2011},
	title = {{Classifier chains for multi-label classification}},
	pages = {333--359},
	volume = {85},
	number = {3},
	issn = {0885-6125},
	journal = {{Machine Learning}},
	doi = {10.1007/s10994-011-5256-5}
}


@book{RitaChattopadhyayJiepingYeSethuramanPanchanathanWeiFanIanDavidson.2011,
	author = {{Rita Chattopadhyay , Jieping Ye, Sethuraman Panchanathan, Wei Fan , Ian Davidson}},
	year = {2011},
	title = {{Multi-Source Domain Adaptation and Its Application to Early Detection of Fatigue}},
	url = {http://dl.acm.org/citation.cfm?id=2020408},
	keywords = {Computer science},
	address = {New York, NY},
	publisher = {ACM},
	isbn = {9781450308137},
	institution = {{ACM Special Interest Group on Knowledge Discovery in Data} and {ACM Special Interest Group on Management of Data}}
}


@article{RonaldL.Iman.,
	author = {Iman, Ronald L. and Davenport, James M.},
	year = {2007},
	title = {{Approximations of the critical region of the friedman statistic}},
	pages = {571--595},
	volume = {9},
	number = {6},
	issn = {0361-0926},
	journal = {{Communications in Statistics - Theory and Methods}},
	doi = {10.1080/03610928008827904}
}




@inproceedings{Yang.,
	author = {Yang, Pei and Tan, Qi and Ding, Yehua},
	title = {{Bayesian Task-Level Transfer Learning for Non-linear Regression}},
	keywords = {Bayesian hierarchical model;RBF network;regression;transfer learning},
	pages = {62--65},
	booktitle = {{2008 International Conference on Computer Science and Software Engineering}},
	doi = {10.1109/CSSE.2008.1612}
}


@book{Xu.2009,
	author = {Xu, Lihong},
	year = {2009},
	title = {{Proceedings of the first ACMSIGEVO Summit on Genetic and Evolutionary Computation}},
	url = {http://dl.acm.org/citation.cfm?id=1543834},
	keywords = {Computer science;maximum margin;transfer learning},
	address = {New York, NY},
	publisher = {ACM},
	isbn = {9781605583266},
	institution = {{ACM Special Interest Group on Genetic and Evolutionary Computation}}
}


@incollection{XavierGlorot.2008,
	author = {{Xavier Glorot} and {Antoine Bordes} and {Yoshua Bengio}},
	title = {{Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach}},
	keywords = {deep learning;domain adaptation;sentiment analysis},
	pages = {1--12},
	publisher = {{John Wiley {\&} Sons, Inc}},
	isbn = {9781118266892},
	editor = {Fabozzi, Frank J. and Kothari, Vinod},
	booktitle = {{Introduction to Securitization}},
	year = {2008},
	address = {Hoboken, NJ, USA},
	doi = {10.1002/9781118266892.ch1}
}


@article{Wu.2017,
	author = {Wu, Qingyao and Zhou, Xiaoming and Yan, Yuguang and Wu, Hanrui and Min, Huaqing},
	year = {2017},
	title = {{Online transfer learning by leveraging multiple source domains}},
	keywords = {multiple source domains;Online learning;Online transfer learning;transfer learning},
	pages = {18},
	volume = {6},
	number = {4},
	issn = {0219-1377},
	journal = {{Knowledge and Information Systems}},
	doi = {10.1007/s10115-016-1021-1}
}


@inproceedings{Williams.2000,
	author = {Williams, Christopher K. I. and Seeger, Matthias},
	title = {{Using the Nystr{\"o}m Method to Speed Up Kernel Machines}},
	url = {http://dl.acm.org/citation.cfm?id=3008751.3008847},
	pages = {661--667},
	publisher = {{MIT Press}},
	series = {NIPS'00},
	booktitle = {{Proceedings of the 13th International Conference on Neural Information Processing Systems}},
	year = {2000},
	address = {Cambridge, MA, USA}
}


@book{WenyuanDai.2007,
	author = {{Wenyuan Dai} and {Gui-Rong Xue} and {Qiang Yang} and {Yong Yu}},
	year = {2007},
	title = {{Co-clustering based Classification for Out-of-domain Documents: Proceedings of the thirteenth ACM SIGKDD international conference on knowledge discovery and data mining, August 12-15, 2007, San Jose, CA, USA}},
	address = {New York, NY},
	publisher = {ACM},
	isbn = {978-1-59593-609-7},
	institution = {{ACM Special Interest Group on Knowledge Discovery in Data} and {ACM Special Interest Group on Management of Data}}
}


@article{WenyuanDai.,
	author = {{Wenyuan Dai} and {Gui-Rong Xue} and {Qiang Yang} and {Yong Yu}},
	title = {{Transferring Naive Bayes Classifiers for Text Classification}},
	keywords = {Machine Learning;Technical Papers}
}


@article{VikasC.Raykar.,
	author = {{Vikas C. Raykar} and {Balaji Krishnapuram} and {Jinbo Bi} and {Murat Dundar} and {R. Bharat Rao}},
	title = {{Bayesian Multiple Instance Learning: Automatic Feature Selection and Inductive Transfer}}
}


@article{UlrichRuckert.,
	author = {{Ulrich R{\"u}ckert}},
	title = {{Kernel Learning and Meta Kernels for Transfer Learning}}
}


@book{TrevorHastie.2009,
	author = {{Trevor Hastie} and {Robert Tibshirani} and {Jerome Friedman}},
	year = {2009},
	title = {{The Elements of Statistical Learning}},
	address = {New York, NY},
	publisher = {{Springer New York}},
	isbn = {978-0-387-84857-0},
	doi = {10.1007/b94608}
}


@article{ThomasG.Dietterich.,
	author = {{Thomas G. Dietterich}},
	title = {{Approximate Statistical Tests for Comparing Supervised Class Learn Algo.pdf}}
}


@book{Suthaharan.2016,
	abstract = {This book presents machine learning models and algorithms to address big data classification problems. Existing machine learning techniques like the decision tree (a hierarchical approach), random forest (an ensemble hierarchical approach), and deep learning (a layered approach) are highly suitable for the system that can handle such problems. This book helps readers, especially students and newcomers to the field of big data and machine learning, to gain a quick understanding of the techniques and technologies; therefore, the theory, examples, and programs (Matlab and R) presented in this book have been simplified, hardcoded, repeated, or spaced for improvements. They provide vehicles to test and understand the complicated concepts of various topics in the field. It is expected that the readers adopt these programs to experiment with the examples, and then modify or write their own programs toward advancing their knowledge for solving more complex and challenging problems. The presentation format of this book focuses on simplicity, readability, and dependability so that both undergraduate and graduate students as well as new researchers, developers, and practitioners in this field can easily trust and grasp the concepts, and learn them effectively. It has been written to reduce the mathematical complexity and help the vast majority of readers to understand the topics and get interested in the field. This book consists of four parts, with the total of 14 chapters. The first part mainly focuses on the topics that are needed to help analyze and understand data and big data. The second part covers the topics that can explain the systems required for processing big data. The third part presents the topics required to understand and select machine learning techniques to classify big data. Finally, the fourth part concentrates on the topics that explain the scaling-up machine learning, an important solution for modern big data problems},
	year = {2016},
	title = {{Machine Learning Models and Algorithms for Big Data Classification // Machine learning models and algorithms for Big Data classification: Thinking with examples for effective learning}},
	address = {New York},
	edition = {1st ed. 2016},
	volume = {36},
	publisher = {{Springer Science+Business Media}},
	isbn = {978-1-4899-7640-6},
	series = {{Integrated series in information systems}},
	editor = {Suthaharan, Shan},
	doi = {10.1007/978-1-4899-7641-3}
}


@book{SparckJones.2003,
	year = {2003},
	title = {{Readings in information retrieval}},
	address = {San Francisco, Calif.},
	edition = {[Nachdr.]},
	publisher = {{Morgan Kaufmann}},
	isbn = {1-55860-454-5},
	series = {{The Morgan Kaufmann series in multimedia information and systems}},
	editor = {{Sparck Jones}, Karen and Willett, Peter}
}




@proceedings{.2008,
	year = {2008},
	title = {{Proceedings of the 23rd National Conference on Artificial Intelligence - Volume 2}},
	publisher = {{AAAI Press}},
	isbn = {978-1-57735-368-3},
	series = {{AAAI'08}}
}


@inproceedings{Pan.2008,
	author = {Pan, Sinno Jialin and Kwok, James T. and Yang, Qiang},
	title = {{Transfer Learning via Dimensionality Reduction}},
	url = {http://dl.acm.org/citation.cfm?id=1620163.1620177},
	pages = {677--682},
	publisher = {{AAAI Press}},
	isbn = {978-1-57735-368-3},
	series = {AAAI'08},
	booktitle = {{Proceedings of the 23rd National Conference on Artificial Intelligence - Volume 2}},
	year = {2008}
}


@inproceedings{Shi.,
	author = {Shi, Xiaoxiao and Liu, Qi and Fan, Wei and Yu, Philip S. and Zhu, Ruixin},
	title = {{Transfer Learning on Heterogenous Feature Spaces via Spectral Transformation}},
	keywords = {feature space;heterogeneous;spectral;transfer learning},
	pages = {1049--1054},
	booktitle = {{2010 IEEE 10th International Conference on Data Mining (ICDM)}},
	doi = {10.1109/ICDM.2010.65}
}


@book{Shaw.2005,
	year = {2005},
	title = {{Introduction to Polymer Viscoelasticity}},
	address = {Hoboken, NJ, USA},
	publisher = {{John Wiley {\&} Sons, Inc}},
	isbn = {9780471741831},
	editor = {Shaw, Montgomery T. and MacKnight, William J.}
}


@book{Shaw.2011,
	year = {2011},
	title = {{Introduction to Polymer Rheology}},
	address = {Hoboken, NJ, USA},
	publisher = {{John Wiley {\&} Sons, Inc}},
	isbn = {9781118170229},
	editor = {Shaw, Montgomery T.},
	doi = {10.1002/9781118170229}
}


@book{Shaw.2011b,
	year = {2011},
	title = {{Introduction to Polymer Rheology}},
	address = {Hoboken, NJ, USA},
	publisher = {{John Wiley {\&} Sons, Inc}},
	isbn = {9781118170229},
	editor = {Shaw, Montgomery T.},
	doi = {10.1002/9781118170229}
}


@article{Liu.,
	author = {Liu, Feng},
	title = {{Heterogeneous Unsupervised Cross-domain Transfer Learning 2017}}
}


@article{LisaTorrey.,
	author = {{Lisa Torrey} and {Jude Shavlik}},
	title = {{Transfer Learning}}
}


@book{Li.2008,
	author = {Li, Ying},
	year = {2008},
	title = {{Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining}},
	url = {http://dl.acm.org/citation.cfm?id=1401890},
	keywords = {Computer science},
	address = {New York, NY},
	publisher = {ACM},
	isbn = {9781605581934},
	institution = {{ACM Special Interest Group on Knowledge Discovery in Data} and {ACM Special Interest Group on Management of Data}}
}


@book{Leskovec.2014,
	author = {Leskovec, Jure and Rajaraman, Anand and Ullman, Jeffrey David},
	year = {2014},
	title = {{Mining of Massive Datasets}},
	address = {Cambridge},
	publisher = {{Cambridge University Press}},
	isbn = {9781139924801},
	doi = {10.1017/CBO9781139924801}
}


@misc{Commenges.,
	abstract = {We give an overview of the role of information theory in statistics, and particularly in biostatistics. We recall the basic quantities in information theory; entropy, cross-entropy, conditional entropy, mutual information and Kullback-Leibler risk. Then we examine the role of information theory in estimation theory, where the log-klikelihood can be identified as being an estimator of a cross-entropy. Then the basic quantities are extended to estimators, leading to criteria for estimator selection, such as Akaike criterion and its extensions. Finally we investigate the use of these concepts in Bayesian theory; the cross-entropy of the predictive distribution can be used for model selection; a cross-validation estimator of this cross-entropy is found to be equivalent to the pseudo-Bayes factor.},
	author = {Commenges, Daniel},
	title = {{Information Theory and Statistics: an overview}},
	url = {http://arxiv.org/pdf/1511.00860v1},
	keywords = {Akaike criterion;cross-entropy;cross-validation;entropy;information theory;Kullback-Leibler risk;likelihood;Mathematics - Statistics;pseudo-Bayes factor;statistical models.;Statistics - Theory}
}


@proceedings{Cohen.,
	title = {the 23rd international conference},
	editor = {Cohen, William and Moore, Andrew}
}


@incollection{ChuongB.Do.2005,
	author = {{Chuong B. Do} and {Andrew Y. Ng}},
	title = {{Transfer learning for text classification}},
	pages = {1--6},
	publisher = {{John Wiley {\&} Sons, Inc}},
	isbn = {9780471741831},
	editor = {Shaw, Montgomery T. and MacKnight, William J.},
	booktitle = {{Introduction to Polymer Viscoelasticity}},
	year = {2005},
	address = {Hoboken, NJ, USA},
	doi = {10.1002/0471741833.ch1}
}


@article{Chu.2016,
	abstract = {Automatic facial action unit (AU) and expression detection from videos is a long-standing problem. The problem is challenging in part because classifiers must generalize to previously unknown subjects that differ markedly in behavior and facial morphology (e.g., heavy versus delicate brows, smooth versus deeply etched wrinkles) from those on which the classifiers are trained. While some progress has been achieved through improvements in choices of features and classifiers, the challenge occasioned by individual differences among people remains. Person-specific classifiers would be a possible solution but for a paucity of training data. Sufficient training data for person-specific classifiers typically is unavailable. This paper addresses the problem of how to personalize a generic classifier without additional labels from the test subject. We propose a transductive learning method, which we refer as a Selective Transfer Machine (STM), to personalize a generic classifier by attenuating person-specific mismatches. STM achieves this effect by simultaneously learning a classifier and re-weighting the training samples that are most relevant to the test subject. We compared STM to both generic classifiers and cross-domain learning methods on four benchmarks: CK+ [44], GEMEP-FERA [67], RU-FACS [4] and GFT [57]. STM outperformed generic classifiers in all.},
	author = {Chu, Wen-Sheng and de {La Torre}, Fernando and Cohn, Jeffrey},
	year = {2016},
	title = {{Selective Transfer Machine for Personalized Facial Expression Analysis}},
	issn = {1939-3539},
	journal = {{IEEE transactions on pattern analysis and machine intelligence}},
	doi = {10.1109/TPAMI.2016.2547397}
}


@article{Chen.2009,
	abstract = {In this paper, a sparse learning algorithm, probabilistic classification vector machines (PCVMs), is proposed. We analyze relevance vector machines (RVMs) for classification problems and observe that adopting the same prior for different classes may lead to unstable solutions. In order to tackle this problem, a signed and truncated Gaussian prior is adopted over every weight in PCVMs, where the sign of prior is determined by the class label, i.e., +1 or -1. The truncated Gaussian prior not only restricts the sign of weights but also leads to a sparse estimation of weight vectors, and thus controls the complexity of the model. In PCVMs, the kernel parameters can be optimized simultaneously within the training algorithm. The performance of PCVMs is extensively evaluated on four synthetic datasets and 13 benchmark datasets using three performance metrics, error rate (ERR), area under the curve of receiver operating characteristic (AUC), and root mean squared error (RMSE). We compare PCVMs with soft-margin support vector machines (SVM(Soft)), hard-margin support vector machines (SVM(Hard)), SVM with the kernel parameters optimized by PCVMs (SVM(PCVM)), relevance vector machines (RVMs), and some other baseline classifiers. Through five replications of twofold cross-validation F test, i.e., 5 x 2 cross-validation F test, over single datasets and Friedman test with the corresponding post-hoc test to compare these algorithms over multiple datasets, we notice that PCVMs outperform other algorithms, including SVM(Soft), SVM(Hard), RVM, and SVM(PCVM), on most of the datasets under the three metrics, especially under AUC. Our results also reveal that the performance of SVM(PCVM) is slightly better than SVM(Soft), implying that the parameter optimization algorithm in PCVMs is better than cross validation in terms of performance and computational complexity. In this paper, we also discuss the superiority of PCVMs' formulation using maximum a posteriori (MAP) analysis and margin analysis, which explain the empirical success of PCVMs.},
	author = {Chen, Huanhuan and Tino, Peter and Yao, Xin},
	year = {2009},
	title = {{Probabilistic classification vector machines}},
	keywords = {Algorithms;Artificial Intelligence;Computer Simulation;Data Interpretation, Statistical;Models, Statistical;Pattern Recognition, Automated/methods},
	pages = {901--914},
	volume = {20},
	number = {6},
	issn = {1045-9227},
	journal = {{IEEE transactions on neural networks}},
	doi = {10.1109/TNN.2009.2014161}
}


@book{Bishop.2009,
	author = {Bishop, Christopher M.},
	year = {2009},
	title = {{Pattern Recognition and Machine Learning // Pattern recognition and machine learning}},
	price = {hbk : EUR 75,92 (freier Pr.)},
	address = {New York, NY},
	edition = {Corrected at 8. printing 2009},
	publisher = {Springer},
	isbn = {0-387-31073-8},
	series = {{Information science and statistics}}
}


@inproceedings{Aytar.,
	author = {Aytar, Yusuf and Zisserman, Andrew},
	title = {{Tabula rasa: Model transfer for object category detection}},
	pages = {2252--2259},
	booktitle = {{2011 IEEE International Conference on Computer Vision (ICCV)}},
	doi = {10.1109/ICCV.2011.6126504}
}


@misc{Ashi.,
	editor = {Ashi},
	title = {{Standardization and Its Effects on K-Means Clustering Algorithmpdf Quelle f{\"u}r Z-Trans}},
	url = {https://www.researchgate.net/publication/288044597_Standardization_and_Its_Effects_on_K-Means_Clustering_Algorithm}
}


@book{AristomenisS.Lampropoulos.2015,
	year = {2015},
	title = {{Machine Learning Paradigms 2015 Einf{\"u}hrung Learnarten // Machine learning paradigms: Applications in recommender systems}},
	address = {Cham},
	volume = {92},
	publisher = {Springer},
	isbn = {978-3-319-19134-8},
	series = {{Intelligent systems reference library}},
	editor = {{Aristomenis S. Lampropoulos} and {George A. Tsihrintzis} and Lampropoulos, Aristomenis S. and Tsihrintzis, George A.}
}


@article{Alpaydm.1999,
	author = {Alpaydm, Ethem},
	year = {1999},
	title = {{Combined 5 $\times$ 2 cv F Test for Comparing Supervised Classification Learning Algorithms}},
	pages = {1885--1892},
	volume = {11},
	number = {8},
	issn = {0899-7667},
	journal = {{Neural Computation}},
	doi = {10.1162/089976699300016007}
}



@article{Abe.2010,
	author = {Abe, Shigeo},
	year = {2010},
	title = {{Support Vector Machines for Pattern Classification}},
	keywords = {1849960976 9781849960977},
	doi = {10.1007/978-1-84996-098-4}
}


@proceedings{.2011,
	year = {2011},
	title = {{Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence}},
	publisher = {{AAAI Press}},
	series = {{AAAI'11}}
}


@proceedings{.2000,
	year = {2000},
	title = {{Proceedings of the 13th International Conference on Neural Information Processing Systems}},
	address = {Cambridge, MA, USA},
	publisher = {{MIT Press}},
	series = {{NIPS'00}}
}


@proceedings{.b,
	title = {{2013 IEEE International Conference on Computer Vision (ICCV)}}
}


@proceedings{.c,
	title = {{2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}}
}


@proceedings{.d,
	title = {{2011 IEEE International Conference on Computer Vision (ICCV)}}
}


@proceedings{.e,
	title = {{2010 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}}
}


@proceedings{.f,
	title = {{2010 IEEE 10th International Conference on Data Mining (ICDM)}}
}


@proceedings{.g,
	title = {{2008 International Conference on Computer Science and Software Engineering}}
}


@proceedings{.h,
	title = {{2008 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}}
}


@inproceedings{Dai.,
	author = {Dai, Wenyuan and Yang, Qiang and Xue, Gui-Rong and Yu, Yong},
	title = {{Boosting for transfer learning}},
	pages = {193--200},
	editor = {Ghahramani, Zoubin},
	booktitle = {the 24th international conference},
	doi = {10.1145/1273496.1273521}
}


@inproceedings{Zhu.2011,
	author = {Zhu, Yin and Chen, Yuqiang and Lu, Zhongqi and Pan, Sinno Jialin and Xue, Gui-Rong and Yu, Yong and Yang, Qiang},
	title = {{Heterogeneous Transfer Learning for Image Classification}},
	url = {http://dl.acm.org/citation.cfm?id=2900423.2900630},
	pages = {1304--1309},
	publisher = {{AAAI Press}},
	series = {AAAI'11},
	booktitle = {{Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence}},
	year = {2011}
}


@article{DanielM.Roy.,
	author = {{Daniel M. Roy} and {Leslie P. Kaelbling}},
	title = {{Efficient Bayesian Task-Level Transfer Learnin}},
	url = {http://dl.acm.org/citation.cfm?id=1625275.1625694},
	keywords = {model-based reasoning;probabilistic reasoning}
}


@proceedings{DiNitto.,
	title = {{the 2015 10th Joint Meeting}},
	editor = {{Di Nitto}, Elisabetta and Harman, Mark and Heymans, Patrick}
}


@book{Kohavi.2004,
	author = {Kohavi, Ron},
	year = {2004},
	title = {{Data Mining in Metric Space: An Empirical Analysis of Supervised Learning Performance Criteria}},
	keywords = {Congresses;Data mining;Database management},
	address = {New York},
	publisher = {{ACM Press}},
	isbn = {1581138881},
	institution = {{Association for Computing Machinery}}
}


@article{Kitayama.2011,
	author = {Kitayama, Satoshi and Yamazaki, Koetsu},
	year = {2011},
	title = {{Simple estimate of the width in Gaussian kernel with adaptive scaling technique}},
	pages = {4726--4737},
	volume = {11},
	number = {8},
	issn = {15684946},
	journal = {{Applied Soft Computing}},
	doi = {10.1016/j.asoc.2011.07.011}
}


@article{katak.,
	author = {katak},
	title = {{A Review of Multi-Label Classification Methods}}
}


@article{JoeyTianyiZhouSinnoJialinPanIvorW.TsangYanYan.,
	author = {{Joey Tianyi Zhou, Sinno Jialin Pan, Ivor W. Tsang, Yan Yan}},
	title = {{Hybrid Heterogeneous Transfer Learning through Deep Learning}},
	keywords = {Novel Machine Learning Algorithms}
}


@book{JingGao.2008,
	author = {{Jing Gao, Wei Fan,Jing Jiang,Jiawei Han}},
	year = {2008},
	title = {{Knowledge Transfer via Multiple Model Local Structure Mapping}},
	url = {http://dl.acm.org/citation.cfm?id=1401890},
	keywords = {Computer science},
	address = {New York, NY},
	publisher = {ACM},
	isbn = {9781605581934},
	institution = {{ACM Special Interest Group on Knowledge Discovery in Data} and {ACM Special Interest Group on Management of Data}}
}


@incollection{JanezDemsar.2006,
	author = {{Janez Demsar}},
	title = {{Statistical Comparisons ofClassifiers over Multiple Data Sets}},
	pages = {1--4},
	publisher = {{John Wiley {\&} Sons, Inc}},
	isbn = {9780471776680},
	editor = {Finlayson, Bruce A.},
	booktitle = {{Introduction to Chemical Engineering Computing}},
	year = {2006},
	address = {Hoboken, NJ, USA},
	doi = {10.1002/0471776688.ch1}
}


@book{Ibanez.2017,
	abstract = {From the table of contents: Part I: Characterization {\&} Modulation of Neurophysiological Signals -- Part II: Empowering {\&} Quantifying Neurorehabilitation -- Part III: Rehabilitation Robotics {\&} Neuroprosthetics
	
	
	
	The book reports on advanced topics in the areas of neurorehabilitation research and practice. It focuses on new methods for interfacing the human nervous system with electronic and mechatronic systems to restore or compensate impaired neural functions. Importantly, the book merges different perspectives, such as the clinical, neurophysiological, and bioengineering ones, to promote, feed and encourage collaborations between clinicians, neuroscientists and engineers. Based on the 2016 International Conference on Neurorehabilitation (ICNR 2016) held on October 18-21, 2016, in Segovia, Spain, this book covers various aspects of neurorehabilitation research and practice, including new insights into biomechanics, brain physiology, neuroplasticity, and brain damages and diseases, as well as innovative methods and technologies for studying and/or recovering brain function, from data mining to interface technologies and neuroprosthetics. In this way, it offers a concise, yet comprehensive reference guide to neurosurgeons, rehabilitation physicians, neurologists, and bioengineers. Moreover, by highlighting current challenges in understanding brain diseases as well as in the available technologies and their implementation, the book is also expected to foster new collaborations between the different groups, thus stimulating new ideas and research directions},
	year = {2017},
	title = {{Converging Clinical and Engineering Research on Neurorehabilitation II: Proceedings of the 3rd International Conference on NeuroRehabilitation (ICNR2016), October 18-21, 2016, Segovia, Spain}},
	url = {http://dx.doi.org/10.1007/978-3-319-46669-9},
	keywords = {Automation;Biomedical engineering;Engineering;Physiotherapy;Rehabilitation medicine;Robotics;User interfaces (Computer systems)},
	address = {Cham and s.l.},
	volume = {15},
	publisher = {{Springer International Publishing}},
	isbn = {9783319466682},
	series = {{Biosystems {\&} Biorobotics}},
	editor = {Ib{\'a}{\~n}ez, Jaime and Gonz{\'a}lez-Vargas, Jos{\'e} and Azor{\'i}n, Jos{\'e} Mar{\'i}a and Akay, Metin and Pons, Jos{\'e} Luis},
	doi = {10.1007/978-3-319-46669-9}
}


@incollection{HalDaumeIII.2011,
	author = {{Hal Daum´e III}},
	title = {{Frustratingly Easy Domain Adaptation}},
	pages = {1--14},
	volume = {1},
	publisher = {{John Wiley {\&} Sons, Inc}},
	isbn = {9781118170229},
	editor = {Shaw, Montgomery T.},
	booktitle = {{Introduction to Polymer Rheology}},
	year = {2011},
	address = {Hoboken, NJ, USA},
	doi = {10.1002/9781118170229.ch1}
}


@article{Habrard.,
	author = {Habrard, Amaury},
	title = {{An Introduction to Transfer Learning and Domain Adaptation}}
}




@proceedings{.2011,
	year = {2011},
	title = {{Proceedings of the 20th International Conference on World Wide Web}},
	address = {New York, NY, USA},
	publisher = {ACM},
	isbn = {978-1-4503-0632-4},
	series = {{WWW '11}}
}


@inproceedings{Qi.2011,
	author = {Qi, Guo-Jun and Aggarwal, Charu and Huang, Thomas},
	title = {{Towards Semantic Knowledge Propagation from Text Corpus to Web Images}},
	url = {http://doi.acm.org/10.1145/1963405.1963449},
	keywords = {cross-domain label propagation;heterogeneous knowledge propagation;text corpus;translator function;web images},
	pages = {297--306},
	publisher = {ACM},
	isbn = {978-1-4503-0632-4},
	series = {WWW '11},
	booktitle = {{Proceedings of the 20th International Conference on World Wide Web}},
	year = {2011},
	address = {New York, NY, USA},
	doi = {10.1145/1963405.1963449}
}





@article{Gretton.2012,
	author = {Gretton, Arthur and Borgwardt, Karsten M. and Rasch, Malte J. and Sch{\"o}lkopf, Bernhard and Smola, Alexander},
	year = {2012},
	title = {{A Kernel Two-sample Test}},
	url = {http://dl.acm.org/citation.cfm?id=2188385.2188410},
	keywords = {hypothesis testing;integral probability metric;kernel methods;schema matching;two-sample test;uniform convergence bounds},
	pages = {723--773},
	volume = {13},
	issn = {1532-4435},
	journal = {{J. Mach. Learn. Res.}}
}




@inproceedings{Gong.,
	author = {Gong, Boqing and Shi, Yuan and Sha, Fei and Grauman, K.},
	title = {{Geodesic flow kernel for unsupervised domain adaptation}},
	pages = {2066--2073},
	booktitle = {{2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}},
	doi = {10.1109/CVPR.2012.6247911}
}


@proceedings{Ghahramani.,
	title = {the 24th international conference},
	editor = {Ghahramani, Zoubin}
}


@book{Finlayson.2006,
	year = {2006},
	title = {{Introduction to Chemical Engineering Computing}},
	address = {Hoboken, NJ, USA},
	publisher = {{John Wiley {\&} Sons, Inc}},
	isbn = {9780471776680},
	editor = {Finlayson, Bruce A.},
	doi = {10.1002/0471776688}
}


@misc{Fernando.,
	abstract = {In this paper, we introduce a new domain adaptation (DA) algorithm where the source and target domains are represented by subspaces spanned by eigenvectors. Our method seeks a domain invariant feature space by learning a mapping function which aligns the source subspace with the target one. We show that the solution of the corresponding optimization problem can be obtained in a simple closed form, leading to an extremely fast algorithm. We present two approaches to determine the only hyper-parameter in our method corresponding to the size of the subspaces. In the first approach we tune the size of subspaces using a theoretical bound on the stability of the obtained result. In the second approach, we use maximum likelihood estimation to determine the subspace size, which is particularly useful for high dimensional data. Apart from PCA, we propose a subspace creation method that outperform partial least squares (PLS) and linear discriminant analysis (LDA) in domain adaptation. We test our method on various datasets and show that, despite its intrinsic simplicity, it outperforms state of the art DA methods.},
	author = {Fernando, Basura and Habrard, Amaury and Sebban, Marc and Tuytelaars, Tinne},
	title = {{Subspace Alignment For Domain Adaptation}},
	url = {http://arxiv.org/pdf/1409.5241v2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}


@article{Fawcett.,
	author = {Fawcett},
	title = {{ROC Graphs:  Notes and Practical Considerations for Data Mining Researchers}},
	year = {2004},
	url = {http://www.hpl.hp.com/techreports/2003/HPL-2003-4.pdf}
}


@incollection{FangtaoLi.2011,
	author = {{Fangtao Li} and {Sinno Jialin Pan} and {Ou Jin} and {Qiang Yang} and {Xiaoyan Zhu}},
	title = {{Introduction}},
	pages = {1--14},
	volume = {1},
	publisher = {{John Wiley {\&} Sons, Inc}},
	isbn = {9781118170229},
	editor = {Shaw, Montgomery T.},
	booktitle = {{Introduction to Polymer Rheology}},
	year = {2011},
	address = {Hoboken, NJ, USA},
	doi = {10.1002/9781118170229.ch1}
}


@article{Fang.2015,
	author = {Fang, Min and Guo, Yong and Zhang, Xiaosong and Li, Xiao},
	year = {2015},
	title = {{Multi-source transfer learning based on label shared subspace}},
	pages = {101--106},
	volume = {51},
	issn = {01678655},
	journal = {{Pattern Recognition Letters}},
	doi = {10.1016/j.patrec.2014.08.011}
}


@book{Fabozzi.2008,
	year = {2008},
	title = {{Introduction to Securitization}},
	address = {Hoboken, NJ, USA},
	publisher = {{John Wiley {\&} Sons, Inc}},
	isbn = {9781118266892},
	editor = {Fabozzi, Frank J. and Kothari, Vinod},
	doi = {10.1002/9781118266892}
}


@article{DonH.JohnsonSinanSinanovic.,
	author = {{Don H. Johnson, Sinan Sinanovi´c}},
	title = {{Symmetrizing the Kullback-Leibler distance}},
	url = {https://www.ece.rice.edu/~dhj/resistor.pdf}
}


@article{Ding.2017,
	abstract = {Metric learning has attracted increasing attention due to its critical role in image analysis and classification. Conventional metric learning always assumes that the training and test data are sampled from the same or similar distribution. However, to build an effective distance metric, we need abundant supervised knowledge (i.e., side/label information), which is generally inaccessible in practice, because of the expensive labeling cost. In this paper, we develop a robust transfer metric learning (RTML) framework to effectively assist the unlabeled target learning by transferring the knowledge from the well-labeled source domain. Specifically, RTML exploits knowledge transfer to mitigate the domain shift in two directions, i.e., sample space and feature space. In the sample space, domain-wise and class-wise adaption schemes are adopted to bridge the gap of marginal and conditional distribution disparities across two domains. In the feature space, our metric is built in a marginalized denoising fashion and low-rank constraint, which make it more robust to tackle noisy data in reality. Furthermore, we design an explicit rank constraint regularizer to replace the rank minimization NP-hard problem to guide the low-rank metric learning. Experimental results on several standard benchmarks demonstrate the effectiveness of our proposed RTML by comparing it with the state-of-the-art transfer learning and metric learning algorithms.},
	author = {Ding, Zhengming and Fu, Yun},
	year = {2017},
	title = {{Robust Transfer Metric Learning for Image Classification}},
	pages = {660--670},
	volume = {26},
	number = {2},
	issn = {1057-7149},
	journal = {{IEEE transactions on image processing : a publication of the IEEE Signal Processing Society}},
	doi = {10.1109/TIP.2016.2631887}
}


@misc{DavidD.Lewis.2004,
	author = {{David D. Lewis}},
	year = {2004},
	title = {{Reuters-21578 text categorization test collection Distribution 1.0: README file (v 1.3)}},
	url = {http://www.daviddlewis.com/resources/testcollections/reuters21578/readme.txt}
}


@article{Zhuang.2012,
	author = {Zhuang, Fuzhen and Luo, Ping and Shen, Zhiyong and He, Qing and Xiong, Yuhong and Shi, Zhongzhi and Xiong, Hui},
	year = {2012},
	title = {{Mining Distinction and Commonality across Multiple Domains Using Generative Model for Text Classification}},
	pages = {2025--2039},
	volume = {24},
	number = {11},
	issn = {1041-4347},
	journal = {{IEEE Transactions on Knowledge and Data Engineering}},
	doi = {10.1109/TKDE.2011.143}
}



@article{Berkes.2011,
	abstract = {The brain maintains internal models of its environment to interpret sensory inputs and to prepare actions. Although behavioral studies have demonstrated that these internal models are optimally adapted to the statistics of the environment, the neural underpinning of this adaptation is unknown. Using a Bayesian model of sensory cortical processing, we related stimulus-evoked and spontaneous neural activities to inferences and prior expectations in an internal model and predicted that they should match if the model is statistically optimal. To test this prediction, we analyzed visual cortical activity of awake ferrets during development. Similarity between spontaneous and evoked activities increased with age and was specific to responses evoked by natural scenes. This demonstrates the progressive adaptation of internal models to the statistics of natural stimuli at the neural level.},
	author = {Berkes, Pietro and Orb{\'a}n, Gergo and Lengyel, M{\'a}t{\'e} and Fiser, J{\'o}zsef},
	year = {2011},
	title = {{Spontaneous cortical activity reveals hallmarks of an optimal internal model of the environment}},
	url = {http://science.sciencemag.org/content/suppl/2011/01/04/331.6013.83.DC1},
	keywords = {Action Potentials;Adaptation, Physiological;Aging;Animals;Bayes Theorem;Darkness;Electrodes, Implanted;Evoked Potentials, Visual;Ferrets;Models, Neurological;Neurons/physiology;Photic Stimulation;Visual Cortex/growth {\&} development/physiology;Visual Perception},
	pages = {83--87},
	volume = {331},
	number = {6013},
	issn = {1095-9203},
	journal = {{Science (New York, N.Y.)}},
	doi = {10.1126/science.1195870}
}

@inproceedings{Yang.1997,
	author = {Yang, Yiming and Pedersen, Jan O.},
	title = {{A Comparative Study on Feature Selection in Text Categorization}},
	url = {http://dl.acm.org/citation.cfm?id=645526.657137},
	pages = {412--420},
	publisher = {{Morgan Kaufmann Publishers Inc}},
	isbn = {1-55860-486-3},
	series = {ICML '97},
	booktitle = {{Proceedings of the Fourteenth International Conference on Machine Learning}},
	year = {1997},
	address = {San Francisco, CA, USA}
}

@incollection{Porter.1997,
	author = {Porter, M. F.},
	title = {{An Algorithm for Suffix Stripping: Readings in Information Retrieval}},
	url = {http://dl.acm.org/citation.cfm?id=275537.275705},
	pages = {313--316},
	publisher = {{Morgan Kaufmann Publishers Inc}},
	isbn = {1-55860-454-5},
	editor = {{Sparck Jones}, Karen and Willett, Peter},
	year = {1997},
	address = {San Francisco, CA, USA},
	booktitle = {Readings in information retrieval}
}


@inproceedings{Saenko.2010,
	author = {Saenko, Kate and Kulis, Brian and Fritz, Mario and Darrell, Trevor},
	title = {{Adapting Visual Category Models to New Domains}},
	url = {http://dl.acm.org/citation.cfm?id=1888089.1888106},
	pages = {213--226},
	publisher = {Springer-Verlag},
	isbn = {3-642-15560-X},
	series = {ECCV'10},
	booktitle = {{Proceedings of the 11th European Conference on Computer Vision: Part IV}},
	year = {2010},
	address = {Berlin, Heidelberg}
}


@incollection{vanBay.2006,
	abstract = {In this paper, we present a novel scale- and rotation-invariant interest point detector and descriptor, coined SURF (Speeded Up Robust Features). It approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster.},
	author = {{van Bay}, Herbert and Tuytelaars Tinne and Gool Luc},
	title = {{SURF: Speeded Up Robust Features}},
	url = {https://doi.org/10.1007/11744023_32},
	pages = {404--417},
	publisher = {{Springer Berlin Heidelberg}},
	isbn = {978-3-540-33833-8},
	editor = {Leonardis, Ale{\v{s}} and Bischof Horst and Pinz Axel},
	booktitle = {{Computer Vision -- ECCV 2006: 9th European Conference on Computer Vision, Graz, Austria, May 7-13, 2006. Proceedings, Part I}},
	year = {2006},
	address = {Berlin, Heidelberg},
	doi = {10.1007/11744023{\textunderscore }32}
}



@misc{Edelman.1991,
	author = {Edelman, Shimon and B{\"u}lthoff, Heinrich H. and Sklar, Erik},
	date = {1991},
	title = {{Task and object learning in visual recognition}},
	url = {http://www.dtic.mil/get-tr-doc/pdf?AD=ADA259961},
	institution = {{MASSACHUSETTS INST OF TECH CAMBRIDGE ARTIFICIAL INTELLIGENCE LAB}}
}


@misc{GregGriffin.,
	author = {{Greg Griffin, Alex Holub, Pietro Perona}},
	title = {{Caltech-256 Object Category Dataset}},
	url = {http://authors.library.caltech.edu/7694/}
}


@article{Lowe.2004,
	author = {Lowe, David G.},
	year = {2004},
	title = {{Distinctive Image Features from Scale-Invariant Keypoints}},
	url = {http://dx.doi.org/10.1023/B:VISI.0000029664.99615.94},
	keywords = {image matching;invariant features;object recognition;scale invariance},
	pages = {91--110},
	volume = {60},
	number = {2},
	issn = {0920-5691},
	journal = {{Int. J. Comput. Vision}},
	doi = {10.1023/B:VISI.0000029664.99615.94}
}


@incollection{Igual.2017,
	abstract = {In this chapter, we will become familiar with descriptive statistics that is comprised of concepts, terms, measures, and tools that help to describe, show, and summarize data in a meaningful way. When analyzing data, it is possible to use both descriptive and inferential statistics in order to analyze the results and draw some conclusions. We will discuss basic concepts, terms, and procedures, such as mean, median, variance, correlation, etc., to explore, describe, and summarize a given set of data.},
	author = {Igual, Laura and Segu{\'i} Santi},
	title = {{Descriptive Statistics}},
	url = {https://doi.org/10.1007/978-3-319-50017-1_3},
	pages = {29--50},
	publisher = {{Springer International Publishing}},
	isbn = {978-3-319-50017-1},
	booktitle = {{Introduction to Data Science: A Python Approach to Concepts, Techniques and Applications}},
	year = {2017},
	address = {Cham},
	doi = {10.1007/978-3-319-50017-1{\textunderscore }3}
}


@book{MaxPlanckGesellschaft.2004,
	year = {2004},
	title = {{Kernel Methods in Computational Biology}},
	address = {Cambridge, MA, USA},
	publisher = {{MIT Press}}
}


@incollection{Vert.2004,
	author = {Vert, JP. and Tsuda, K. and Sch{\"o}lkopf, B.},
	title = {{A Primer on Kernel Methods}},
	pages = {35--70},
	publisher = {{MIT Press}},
	booktitle = {{Kernel Methods in Computational Biology}},
	year = {2004},
	address = {Cambridge, MA, USA}
}

@article{Duan.2012,
	author = {Duan, L. and Xu, D. and Tsang, I. W. H. and Luo, J.},
	year = {2012},
	title = {{Visual Event Recognition in Videos by Learning from Web Data}},
	keywords = {adapted target classifier;adaptive MKL;adaptive multiple kernel learning;aligned space-time pyramid matching;aligned space-time pyramid matching.;A-MKL;Automated;consumer video domain;cross-domain learning;data distribution;distance measurement;domain adaptation;Event recognition;feature distribution;Feature extraction;Human Activities;Humans;image classification;image matching;information fusion;Internet;Kernel;labeled consumer video;learning (artificial intelligence);learning algorithm convergence;Learning systems;loosely labeled Web video;mismatch minimisation;Pattern Recognition;prelearned average classifier;prelearned classifier;pyramid level;Reproducibility of Results;structural risk functional minimisation;support vector machines;SVM classifier;transfer learning;transfer learning method;Video Recording;video signal processing;Videos;visual event recognition;Visualization;Web data;web data leveraging;Web video domain;YouTube},
	pages = {1667--1680},
	volume = {34},
	number = {9},
	issn = {1939-3539},
	journal = {{IEEE transactions on pattern analysis and machine intelligence}},
	doi = {10.1109/TPAMI.2011.265}
}

@article{Chai.2014,
	author = {Chai, T. and Draxler, R. R.},
	year = {2014},
	title = {{Root mean square error (RMSE) or mean absolute error (MAE)? - Arguments against avoiding RMSE in the literature}},
	url = {https://www.geosci-model-dev.net/7/1247/2014/},
	pages = {1247--1250},
	volume = {7},
	number = {3},
	journal = {{Geoscientific Model Development}},
	doi = {10.5194/gmd-7-1247-2014}
}

@book{Teschl.2014,
	author = {Teschl, Gerald and Teschl, Susanne},
	year = {2014},
	title = {{Mathematik f{\"u}r Informatiker: Bd. 2: Analysis und Statistik}},
	price = {kart. : EUR 27.99 (DE)},
	address = {Berlin and Heidelberg},
	edition = {3., {\"u}berarb. Aufl.},
	publisher = {{Springer Vieweg}},
	isbn = {978-3-642-54273-2}
}

@article{Dietterich.1998,
	author = {Dietterich, Thomas G.},
	year = {1998},
	title = {{Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms}},
	url = {http://dx.doi.org/10.1162/089976698300017197},
	pages = {1895--1923},
	volume = {10},
	number = {7},
	issn = {0899-7667},
	journal = {{Neural Comput}},
	doi = {10.1162/089976698300017197}
}


@book{Hinton.2004,
	abstract = {This book provides the student with all that they need to undertake statistical analysis using SPSS, guiding them from the basic rationale behind the statistics, through detailed explanations of the procedures, to all aspects of the SPSS output},
	author = {Hinton, Perry R.},
	year = {2004},
	title = {{SPSS explained}},
	address = {London},
	publisher = {Routledge},
	isbn = {0415274095}
}

@book{Bortz.2010,
	author = {Bortz, J{\"u}rgen and Schuster, Christof},
	year = {2010},
	title = {{Statistik f{\"u}r Human- und Sozialwissenschaftler}},
	address = {Berlin, Heidelberg},
	edition = {7., vollst{\"a}ndig {\"u}berarbeitete und erweiterte Auflage},
	publisher = {{Springer-Verlag Berlin Heidelberg}},
	isbn = {978-3-642-12769-4},
	series = {{Springer-Lehrbuch}},
	doi = {10.1007/978-3-642-12770-0}
}


@article{Tipping.2001,
	author = {Tipping, Michael E.},
	year = {2001},
	title = {{Sparse Bayesian Learning and the Relevance Vector Machine}},
	url = {http://dx.doi.org/10.1162/15324430152748236},
	pages = {211--244},
	volume = {1},
	issn = {1532-4435},
	journal = {{J. Mach. Learn. Res.}},
	doi = {10.1162/15324430152748236}
}

@proceedings{.1997,
	year = {1997},
	title = {{Advances in Neural Information Processing Systems 9}},
	publisher = {{MIT Press}}
}


@inproceedings{Burges.1997,
	author = {Burges, Chris J. C. and Sch{\"o}lkopf, Bernhard},
	title = {{Improving the Accuracy and Speed of Support Vector Machines}},
	pages = {375--381},
	publisher = {{MIT Press}},
	booktitle = {{Advances in Neural Information Processing Systems 9}},
	year = {1997}
}

@book{Dorronsoro.2002,
	year = {2002},
	title = {{Artificial Neural Networks --- ICANN 2002: International Conference Madrid, Spain, August 28--30, 2002 Proceedings}},
	address = {Berlin, Heidelberg},
	publisher = {{Springer Berlin Heidelberg}},
	isbn = {978-3-540-46084-8},
	editor = {Dorronsoro, Jos{\'e} R.}
}


@incollection{Graepel.2002,
	abstract = {We consider the problem of missing data in kernel-based learning algorithms. We explain how semidefinite programming can be used to perform an approximate weighted completion of the kernel matrix that ensures positive semidefiniteness and hence Mercer's condition. In numerical experiments we apply a support vector machine to the XOR classification task based on randomly sparsified kernel matrices from a polynomial kernel of degree 2. The approximate completion algorithm leads to better generalisation and to fewer support vectors as compared to a simple spectral truncation method at the cost of considerably longer runtime. We argue that semidefinite programming provides an interesting convex optimisation framework for machine learning in general and for kernel-machines in particular.},
	author = {Graepel, Thore},
	title = {{Kernel Matrix Completion by Semidefinite Programming}},
	url = {https://doi.org/10.1007/3-540-46084-5_113},
	pages = {694--699},
	publisher = {{Springer Berlin Heidelberg}},
	isbn = {978-3-540-46084-8},
	editor = {Dorronsoro, Jos{\'e} R.},
	booktitle = {{Artificial Neural Networks --- ICANN 2002: International Conference Madrid, Spain, August 28--30, 2002 Proceedings}},
	year = {2002},
	address = {Berlin, Heidelberg},
	doi = {10.1007/3-540-46084-5{\textunderscore }113}
}




@book{Theodoridis.2015,
	abstract = {Includes bibliographical references and index},
	author = {Theodoridis, Sergios},
	year = {2015},
	title = {{Machine learning: A Bayesian and optimization perspective}},
	price = {Print},
	address = {London},
	edition = {First edition},
	publisher = {Elsevier},
	isbn = {978-0-12-801522-3}
}




@incollection{Borsboom.,
	author = {Borsboom, Denny and Mellenbergh, Gideon J. and {van Heerden}, Jaap},
	title = {{The theoretical status of latent variables}},
	pages = {203--219},
	volume = {110},
	booktitle = {{Psychological Review}},
	doi = {10.1037/0033-295X.110.2.203},
	year ={2003}
}





@article{Albert.1993,
	abstract = {A vast literature in statistics, biometrics, and econometrics is concerned with the analysis of binary and polychotomous response data. The classical approach fits a categorical response regression model using maximum likelihood, and inferences about the model are based on the associated asymptotic theory. The accuracy of classical confidence statements is questionable for small sample sizes. In this article, exact Bayesian methods for modeling categorical response data are developed using the idea of data augmentation. The general approach can be summarized as follows. The probit regression model for binary outcomes is seen to have an underlying normal regression structure on latent continuous data. Values of the latent data can be simulated from suitable truncated normal distributions. If the latent data are known, then the posterior distribution of the parameters can be computed using standard results for normal linear models. Draws from this posterior are used to sample new latent data, and the process is iterated with Gibbs sampling. This data augmentation approach provides a general framework for analyzing binary regression models. It leads to the same simplification achieved earlier for censored regression models. Under the proposed framework, the class of probit regression models can be enlarged by using mixtures of normal distributions to model the latent data. In this normal mixture class, one can investigate the sensitivity of the parameter estimates to the choice of {\dq}link function,{\dq} which relates the linear regression estimate to the fitted probabilities. In addition, this approach allows one to easily fit Bayesian hierarchical models. One specific model considered here reflects the belief that the vector of regression coefficients lies on a smaller dimension linear subspace. The methods can also be generalized to multinomial response models with J {\textgreater} 2 categories. In the ordered multinomial model, the J categories are ordered and a model is written linking the cumulative response probabilities with the linear regression structure. In the unordered multinomial model, the latent variables have a multivariate normal distribution with unknown variance-covariance matrix. For both multinomial models, the data augmentation method combined with Gibbs sampling is outlined. This approach is especially attractive for the multivariate probit model, where calculating the likelihood can be difficult.},
	author = {Albert, James H. and Chib, Siddhartha},
	year = {1993},
	title = {{Bayesian Analysis of Binary and Polychotomous Response Data}},
	url = {http://www.jstor.org/stable/2290350},
	pages = {669--679},
	volume = {88},
	number = {422},
	issn = {01621459},
	journal = {{Journal of the American Statistical Association}}
}



@article{Dempster.1977,
	abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
	author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
	year = {1977},
	title = {{Maximum Likelihood from Incomplete Data via the EM Algorithm}},
	url = {http://www.jstor.org/stable/2984875},
	pages = {1--38},
	volume = {39},
	number = {1},
	issn = {00359246},
	journal = {{Journal of the Royal Statistical Society. Series B (Methodological)}}
}



@article{CaroLopera.2012,
	author = {Caro-Lopera, Francisco J. and Leiva, V{\'i}ctor and Balakrishnan, N.},
	year = {2012},
	title = {{Connection between the Hadamard and matrix products with an application to matrix-variate Birnbaum--Saunders distributions}},
	url = {http://www.sciencedirect.com/science/article/pii/S0047259X11001461},
	keywords = {Generalized Birnbaum--Saunders distribution;Kronecker product;Multivariate analysis;Schur or entry-wise product;Shape theory},
	pages = {126--139},
	volume = {104},
	number = {1},
	issn = {0047-259X},
	journal = {{Journal of Multivariate Analysis}},
	doi = {10.1016/j.jmva.2011.07.004}
}



@proceedings{MilanStudeny.2006,
	year = {2006},
	title = {{Third European Workshop on Probabilistic Graphical Models, 12-15 September  2006, Prague, Czech Republic. Electronic Proceedings}},
	editor = {{Milan Studen{\'y}} and {Jir$\backslash$'{\i} Vomlel}}
}


@inproceedings{YiWang.2006,
	author = {{Yi Wang} and {Nevin Lianwen Zhang}},
	title = {{Severity of Local Maxima for the EM Algorithm: Experiences with  Hierarchical Latent Class Models  2006, Prague, Czech Republic. Electronic Proceedings}},
	url = {http://www.utia.cas.cz/files/mtr/pgm06/9_paper.pdf},
	pages = {301--308},
	editor = {{Milan Studen{\'y}} and {Jir$\backslash$'{\i} Vomlel}},
	booktitle = {{Third European Workshop on Probabilistic Graphical Models, 12-15 September  2006, Prague, Czech Republic. Electronic Proceedings}},
	year = {2006}
}



@article{Chen.2014,
	author = {Chen, H. and Tino, P. and Yao, X.},
	year = {2014},
	title = {{Efficient Probabilistic Classification Vector Machine With Incremental Basis Function Selection}},
	keywords = {1;Algorithms;Approximation algorithms;Approximation methods;Artificial Intelligence;Automated;Bayes methods;Bayesian classification;Bayesian estimation;computational effectiveness;Decision Support Techniques;efficient PCVM;efficient probabilistic classification model;EPCVM;expectation maximization algorithm;expectation propagation;expectation propagation (EP);expectation-maximisation algorithm;generalization performance;Humans;hybrid Monte Carlo approach;incremental basis function selection;incremental learning;Laplace approximation;Laplace equations;learning (artificial intelligence);marginal likelihood maximization;Monte Carlo methods;pattern classification;Pattern Recognition;probabilistic classification vector machine;Probabilistic logic;probability;Rademacher complexity;relevance vector machine;sparse learning approach;stability;stability problems;Support Vector Machine;support vector machine (SVM);support vector machines;Training;Vectors},
	pages = {356--369},
	volume = {25},
	number = {2},
	issn = {2162-237X},
	journal = {{IEEE Transactions on Neural Networks and Learning Systems}},
	doi = {10.1109/TNNLS.2013.2275077}
}




@inproceedings{Schleif.2015,
	author = {Schleif, F. M. and Chen, H. and Tino, P.},
	title = {{Incremental probabilistic classification vector machine with linear costs}},
	keywords = {asymmetric matrices;computational complexity;incremental probabilistic classification vector machine;linear runtime;memory complexity;Nystr{\"o}m approximation;pattern classification;probability;support vector machines;Xenon},
	pages = {1--8},
	booktitle = {{2015 International Joint Conference on Neural Networks (IJCNN)}},
	year = {2015},
	doi = {10.1109/IJCNN.2015.7280377}
}





@book{Bishop.1995,
	author = {Bishop, Christopher M.},
	year = {1995},
	title = {{Neural Networks for Pattern Recognition}},
	address = {New York, NY, USA},
	publisher = {{Oxford University Press, Inc}},
	isbn = {0198538642}
}




@article{Figueiredo.2003,
	author = {Figueiredo, M. A. T.},
	year = {2003},
	title = {{Adaptive sparseness for supervised learning}},
	keywords = {adaptive sparseness;Automatic control;Bayes methods;Bayesian approaches;Bayesian methods;benchmark datasets;expectation-maximization algorithm;experiments;Feedforward neural networks;functional mapping;generalisation (artificial intelligence);generalization;hierarchical-Bayes interpretation;hyperparameters;inference mechanisms;Kernel;Laplace equations;Laplacian prior;learning by example;maximum likelihood estimation;Neural networks;noninformative hyperprior;sparse solutions;statistical analysis;supervised learning;Support vector machine classification;support vector machines;Training data;training examples},
	pages = {1150--1159},
	volume = {25},
	number = {9},
	issn = {1939-3539},
	journal = {{IEEE transactions on pattern analysis and machine intelligence}},
	doi = {10.1109/TPAMI.2003.1227989}
}



@book{Press.2007,
	author = {Press, William H. and Teukolsky, Saul A. and Vetterling, William T. and Flannery, Brian P.},
	year = {2007},
	title = {{Numerical Recipes 3rd Edition: The Art of Scientific Computing}},
	address = {New York, NY, USA},
	edition = {3},
	publisher = {{Cambridge University Press}},
	isbn = {0521880688}
}




@article{Eltoft.2006,
	author = {Eltoft, T. and Kim, Taesu and Lee, Te-Won},
	year = {2006},
	title = {{On the multivariate Laplace distribution}},
	keywords = {Context modeling;discrete Fourier transform coefficient;discrete Fourier transforms;Discrete wavelet transforms;Gaussian distribution;Gaussian processes;Independent component analysis;Laplace transforms;Multidimensional Laplace distribution;multivariate Laplace distribution;normal variance mixture model;parameter estimation;probability;probability density function;scale mixture of Gaussians model;Speech;speech processing;speech signal;Statistical distributions;statistical modeling;statistical representation},
	pages = {300--303},
	volume = {13},
	number = {5},
	issn = {1070-9908},
	journal = {{IEEE Signal Processing Letters}},
	doi = {10.1109/LSP.2006.870353}
}


@article{Mohamad.2013,
	author = {Mohamad, Ismail Bin and Usman, Dauda},
	year = {2013},
	title = {{Standardization and its effects on K-means clustering algorithm}},
	pages = {3299--3303},
	volume = {6},
	number = {17},
	journal = {{Research Journal of Applied Sciences, Engineering and Technology}}
}



@book{Scholkopf.2001,
	author = {Scholkopf, Bernhard and Smola, Alexander J.},
	year = {2001},
	title = {{Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond}},
	address = {Cambridge, MA, USA},
	publisher = {{MIT Press}},
	isbn = {0262194759}
}



@proceedings{.2008,
	year = {2008},
	title = {{Proceedings of the 25th International Conference on Machine Learning}},
	address = {New York, NY, USA},
	publisher = {ACM},
	isbn = {978-1-60558-205-4},
	series = {{ICML '08}}
}


@inproceedings{Zhang.2008,
	author = {Zhang, Kai and Tsang, Ivor W. and Kwok, James T.},
	title = {{Improved Nystr{\"o}m Low-rank Approximation and Error Analysis}},
	url = {http://doi.acm.org/10.1145/1390156.1390311},
	pages = {1232--1239},
	publisher = {ACM},
	isbn = {978-1-60558-205-4},
	series = {ICML '08},
	booktitle = {{Proceedings of the 25th International Conference on Machine Learning}},
	year = {2008},
	address = {New York, NY, USA},
	doi = {10.1145/1390156.1390311}
}


@article{Nemtsov.2016,
	author = {Nemtsov, Arik and Averbuch, Amir and Schclar, Alon},
	year = {2016},
	title = {{Matrix compression using the Nystr{\"o}m method}},
	keywords = {evd;nystr{\"o}m;out-of-sample extension;svd},
	pages = {997--1019},
	volume = {20},
	number = {5},
	issn = {1088467X},
	journal = {{Intelligent Data Analysis}},
	doi = {10.3233/IDA-160854}
}




@article{He.2017,
	author = {He, Li and Ray, Nilanjan and Zhang, Hong},
	year = {2017},
	title = {{Error bound of Nystr{\"o}m-approximated NCut eigenvectors and its application to training size selection}},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231217302813},
	keywords = {Error bound;Kernel matrix;Nystr{\"o}m approximation;Pattern Recognition;Singular values},
	pages = {130--142},
	volume = {239},
	issn = {0925-2312},
	journal = {{Neurocomputing}},
	doi = {10.1016/j.neucom.2017.02.011}
}



@article{Kumar.2012,
	author = {Kumar, Sanjiv and Mohri, Mehryar and Talwalkar, Ameet},
	year = {2012},
	title = {{Sampling Methods for the Nystr{\"o}m Method}},
	url = {http://dl.acm.org/citation.cfm?id=2188385.2343678},
	keywords = {ensemble methods;large-scale learning;low-rank approximation;nystr{\"o}m method},
	pages = {981--1006},
	volume = {13},
	issn = {1532-4435},
	journal = {{J. Mach. Learn. Res.}}
}



@article{Li.2015,
	author = {Li, M. and Bi, W. and Kwok, J. T. and Lu, B. L.},
	year = {2015},
	title = {{Large-Scale Nystr{\"o}m Kernel Matrix Approximation Using Randomized SVD}},
	pages = {152--164},
	volume = {26},
	number = {1},
	issn = {2162-237X},
	journal = {{IEEE Transactions on Neural Networks and Learning Systems}},
	doi = {10.1109/TNNLS.2014.2359798}
}



@article{Johnson.2008,
	author = {Johnson, R. and Zhang, T.},
	year = {2008},
	title = {{Graph-Based Semi-Supervised Learning and Spectral Kernel Design}},
	keywords = {Concrete;Design methodology;graph theory;graph-based semisupervised learning;Graph-based semi-supervised learning;Information processing;Kernel;kernel design;learning (artificial intelligence);learning bounds;Pattern Recognition;Performance analysis;Semisupervised learning;spectral decomposition;spectral kernel design;Statistical learning;Statistics;supervised learning;transductive learning;unsupervised kernel design},
	pages = {275--288},
	volume = {54},
	number = {1},
	issn = {0018-9448},
	journal = {{IEEE Transactions on Information Theory}},
	doi = {10.1109/TIT.2007.911294}
}

% This file was created with Citavi 5.4.0.2

@inproceedings{KaiZhang.2013,
	abstract = {Covariate shift is a unconventional learning scenario in which training and testing data have different distributions. A general principle to solve the problem is to make the training data distribution similar to the test one, such that classifiers computed on the former generalizes well to the latter. Current approaches typically target on the sample distribution in the input space, however, for kernel-based learning methods, the algorithm performance depends directly on the geometry of the kernel-induced feature space. Motivated by this, we propose to match data distributions in the Hilbert space, which, given a pre-defined empirical kernel map, can be formulated as aligning kernel matrices across domains. In particular, to evaluate similarity of kernel matrices defined on arbitrarily different samples, the novel concept of surrogate kernel is introduced based on the Mercers theorem. Our approach caters the model adaptation specifically to kernel-based learning mechanism, and demonstrates promising results on several real-world applications.},
	author = {{Kai Zhang} and {Vincent Zheng} and {Qiaojun Wang} and {James Kwok} and {Qiang Yang} and {Ivan Marsic}},
	title = {{Covariate Shift in Hilbert Space: A Solution via Sorrogate Kernels}},
	url = {http://jmlr.org/proceedings/papers/v28/zhang13b.pdf},
	pages = {388--395},
	volume = {28},
	publisher = {{JMLR Workshop and Conference Proceedings}},
	editor = {{Sanjoy Dasgupta} and {David Mcallester}},
	booktitle = {{Proceedings of the 30th International Conference on Machine Learning (ICML-13)}},
	year = {2013}
}


@proceedings{SanjoyDasgupta.2013,
	year = {2013},
	title = {{Proceedings of the 30th International Conference on Machine Learning (ICML-13)}},
	publisher = {{JMLR Workshop and Conference Proceedings}},
	editor = {{Sanjoy Dasgupta} and {David Mcallester}}
}

@inproceedings{Mihail.2002,
	author = {Mihail, Milena and Papadimitriou, Christos H.},
	title = {{On the Eigenvalue Power Law}},
	url = {http://dl.acm.org/citation.cfm?id=646978.711701},
	pages = {254--262},
	publisher = {Springer-Verlag},
	isbn = {3-540-44147-6},
	series = {RANDOM '02},
	booktitle = {{Proceedings of the 6th International Workshop on Randomization and Approximation Techniques}},
	year = {2002},
	address = {London, UK, UK}
}

% This file was created with Citavi 5.4.0.2

@book{Gentle.2007,
	author = {Gentle, James E.},
	year = {2007},
	title = {{Matrix Algebra: Theory, Computations, and Applications in Statistics}},
	address = {New York, NY},
	publisher = {{Springer Science+Business Media LLC}},
	isbn = {978-0-387-70872-0},
	series = {{Springer Texts in Statistics}},
	doi = {10.1007/978-0-387-70873-7}
}


% This file was created with Citavi 5.4.0.2

@book{Kobti.2007,
	author = {Kobti, Ziad and Wu, Dan},
	year = {2007},
	title = {{Advances in Artificial Intelligence: 20th Conference of the Canadian Society for Computational Studies of Intelligence, Canadian AI 2007, Montreal, Canada, May 28-30, 2007. Proceedings}},
	address = {Berlin, Heidelberg},
	volume = {4509},
	publisher = {{Springer-Verlag Berlin Heidelberg}},
	isbn = {978-3-540-72665-4},
	series = {{Lecture Notes in Computer Science}},
	institution = {{Canadian AI 2007} and {Conference of the Canadian Society for Computational Studies of Intelligence, CSCSI} and {AI 2007}},
	doi = {10.1007/978-3-540-72665-4}
}

% This file was created with Citavi 5.4.0.2

@article{Chattopadhyay.2012,
	author = {Chattopadhyay, Rita and Sun, Qian and Fan, Wei and Davidson, Ian and Panchanathan, Sethuraman and Ye, Jieping},
	year = {2012},
	title = {{Multisource Domain Adaptation and Its Application to Early Detection of Fatigue}},
	url = {http://doi.acm.org/10.1145/2382577.2382582},
	keywords = {Multisource domain adaption;subject-based variability;surface electromyogram;transfer learning},
	pages = {18:1--18:26},
	volume = {6},
	number = {4},
	issn = {1556-4681},
	journal = {{ACM Trans. Knowl. Discov. Data}},
	doi = {10.1145/2382577.2382582}
}

% This file was created with Citavi 5.4.0.2

@article{Weiss.2016,
	author = {Weiss, Karl and Khoshgoftaar, Taghi M. and Wang, DingDing},
	year = {2016},
	title = {{A survey of transfer learning}},
	keywords = {Transfer learning Survey Domain adaptation Machine learning Data mining},
	pages = {1817},
	volume = {3},
	number = {1},
	issn = {2196-1115},
	journal = {{Journal of Big Data}},
	doi = {10.1186/s40537-016-0043-6}
}

% This file was created with Citavi 5.4.0.2

@book{Theodoridis.2008,
	author = {Theodoridis, Sergios and Koutroumbas, Konstantinos},
	year = {2008},
	title = {{Pattern Recognition, Fourth Edition}},
	edition = {4th},
	publisher = {{Academic Press}},
	isbn = {1597492728}
}

% This file was created with Citavi 5.4.0.2

@book{Aggarwal.2015,
	year = {2015},
	title = {{Data classification: Algorithms and applications}},
	price = {(Print)},
	address = {Boca Raton, FL},
	publisher = {{CRC Press}},
	isbn = {9781466586741},
	series = {{Chapman {\&} Hall / CRC data mining and knowledge discovery series}},
	editor = {Aggarwal, Charu C.}
}


% This file was created with Citavi 5.4.0.2

@proceedings{.2007,
	year = {2007},
	title = {{Seventh IEEE International Conference on Data Mining Workshops (ICDMW 2007)}}
}


@inproceedings{Arnold.2007,
	author = {Arnold, A. and Nallapati, R. and Cohen, W. W.},
	title = {{A Comparative Study of Methods for Transductive Transfer Learning}},
	keywords = {biology computing;Conferences;Data mining;Encyclopedias;entropy;inductive approaches;iterative feature transformation;iterative methods;learning by example;Machine Learning;maximum entropy based technique;maximum entropy methods;probability;protein name extraction;proteins;support vector machines;Testing;Training data;transductive support vector machine;unsupervised learning;unsupervised transductive transfer learning},
	pages = {77--82},
	booktitle = {{Seventh IEEE International Conference on Data Mining Workshops (ICDMW 2007)}},
	year = {2007},
	doi = {10.1109/ICDMW.2007.109}
}

% This file was created with Citavi 5.4.0.2

@book{QuinoneroCandela.2009,
	year = {2009},
	title = {{Dataset shift in machine learning}},
	address = {Cambridge, Mass},
	publisher = {{MIT Press}},
	isbn = {9780262170055},
	series = {{Neural information processing series}},
	editor = {Qui{\~n}onero-Candela, Joaquin}
}



@proceedings{.2008,
	year = {2008},
	title = {{Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases - Part II}},
	address = {Berlin, Heidelberg},
	publisher = {Springer-Verlag},
	isbn = {978-3-540-87480-5},
	series = {{ECML PKDD '08}}
}


@inproceedings{Wang.2008,
	author = {Wang, Zheng and Song, Yangqiu and Zhang, Changshui},
	title = {{Transferred Dimensionality Reduction}},
	url = {http://dx.doi.org/10.1007/978-3-540-87481-2_36},
	keywords = {Clustering;Dimensionality Reduction;transfer learning},
	pages = {550--565},
	publisher = {Springer-Verlag},
	isbn = {978-3-540-87480-5},
	series = {ECML PKDD '08},
	booktitle = {{Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases - Part II}},
	year = {2008},
	address = {Berlin, Heidelberg},
	doi = {10.1007/978-3-540-87481-2{\textunderscore }36}
}



@article{Huang.2012,
	author = {Huang, Pipei and Wang, Gang and Qin, Shiyin},
	year = {2012},
	title = {{Boosting for Transfer Learning from Multiple Data Sources}},
	url = {http://dx.doi.org/10.1016/j.patrec.2011.11.023},
	keywords = {Boosting;multiple source domains;Opinion mining;Sentimental classification;transfer learning;Transfer learning with multiple sources},
	pages = {568--579},
	volume = {33},
	number = {5},
	issn = {01678655},
	journal = {{Pattern Recognition Letters}},
	doi = {10.1016/j.patrec.2011.11.023}
}




@article{Golub.1965,
	abstract = {A numerically stable and fairly fast scheme is described to compute the unitary matrices U and V which transform a given matrix A into a diagonal form $\backslash$$\backslash$Sigma = U{\^{}} * AV, thus exhibiting A's singular values on $\backslash$$\backslash$Sigma 's diagonal. The scheme first transforms A to a bidiagonal matrix J, then diagonalizes J. The scheme described here is complicated but does not suffer from the computational difficulties which occasionally afflict some previously known methods. Some applications are mentioned, in particular the use of the pseudo-inverse A{\^{}}I = V$\backslash$$\backslash$Sigma {\^{}}I U{\^{}}*  to solve least squares problems in a way which dampens spurious oscillation and cancellation.},
	author = {Golub, G. and Kahan, W.},
	year = {1965},
	title = {{Calculating the Singular Values and Pseudo-Inverse of a Matrix}},
	keywords = {algebra;numerical;svd},
	pages = {205--224},
	volume = {2},
	number = {2},
	journal = {{Journal of the Society for Industrial and Applied Mathematics, Series B: Numerical Analysis}},
	doi = {10.1137/0702016}
}



@misc{Gong.2015,
	author = {Gong, Boqing},
	year = {2015},
	title = {{Kernel Methods for Unsupervised Domain Adaptation}},
	url = {http://www-scf.usc.edu/~boqinggo/}
}



@proceedings{.2012,
	year = {2012},
	title = {{Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1}},
	address = {Stroudsburg, PA, USA},
	publisher = {{Association for Computational Linguistics}},
	series = {{ACL '12}}
}


@inproceedings{Li.2012,
	author = {Li, Fangtao and Pan, Sinno Jialin and Jin, Ou and Yang, Qiang and Zhu, Xiaoyan},
	title = {{Cross-domain Co-extraction of Sentiment and Topic Lexicons}},
	url = {http://dl.acm.org/citation.cfm?id=2390524.2390582},
	pages = {410--419},
	publisher = {{Association for Computational Linguistics}},
	series = {ACL '12},
	booktitle = {{Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1}},
	year = {2012},
	address = {Stroudsburg, PA, USA}
}




@inproceedings{Tommasi.,
	author = {Tommasi, Tatiana and Orabona, Francesco and Caputo, Barbara},
	title = {{Safety in numbers: Learning categories from few examples with multi model knowledge transfer}},
	pages = {3081--3088},
	booktitle = {{2010 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}},
	doi = {10.1109/CVPR.2010.5540064}
}



@inproceedings{Tommasi.2009,
	author = {Tommasi, Tatiana and Caputo, Barbara},
	title = {{The more you know, the less you learn: From knowledge transfer to one-shot learning of object categories}},
	pages = {80.1--80.11},
	publisher = {{BMVA Press}},
	isbn = {1-901725-39-1},
	booktitle = {{Proceedings of the British Machine Vision Conference}},
	year = {2009},
	doi = {10.5244/C.23.80}
}



@article{Evgeniou.2004,
	abstract = {We study the leave-one-out and generalization errors of voting combinations of learning machines. A special case considered is a variant of bagging. We analyze in detail combinations of kernel machines, such as support vector machines, and present theoretical estimates of their leave-one-out error. We also derive novel bounds on the stability of combinations of any classifiers. These bounds can be used to formally show that, for example, bagging increases the stability of unstable learning machines. We report experiments supporting the theoretical findings.},
	author = {Evgeniou, Theodoros and Pontil, Massimiliano and Elisseeff, Andr{\'e}},
	year = {2004},
	title = {{Leave One Out Error, Stability, and Generalization of Voting Combinations of Classifiers}},
	url = {https://doi.org/10.1023/B:MACH.0000019805.88351.60},
	pages = {71--97},
	volume = {55},
	number = {1},
	issn = {0885-6125},
	journal = {{Machine Learning}},
	doi = {10.1023/B:MACH.0000019805.88351.60}
}



@article{Ma.1994,
	author = {Ma, Changxue and Kamp, Y. and Willems, L. F.},
	year = {1994},
	title = {{A Frobenius norm approach to glottal closure detection from the speech signal}},
	keywords = {Acoustic noise;algorithm;computational efficiency;Filters;Frobenius norm approach;glottal closure detection;least squares approximations;Linear predictive coding;Predictive models;processing;Pulse modulation;robustness;singular value decomposition;speech analysis;Speech coding;speech processing;speech prosody manipulation;speech signal;speech synthesis;SVD method;total linear least squares technique;voice source analysis},
	pages = {258--265},
	volume = {2},
	number = {2},
	issn = {1063-6676},
	journal = {{IEEE Transactions on Speech and Audio Processing}},
	doi = {10.1109/89.279274}
}



@proceedings{.2005,
	year = {2005},
	title = {{Proceedings of the 22Nd International Conference on Machine Learning}},
	address = {New York, NY, USA},
	publisher = {ACM},
	isbn = {1-59593-180-5},
	series = {{ICML '05}}
}


@inproceedings{Rennie.2005,
	author = {Rennie, Jasson D. M. and Srebro, Nathan},
	title = {{Fast Maximum Margin Matrix Factorization for Collaborative Prediction}},
	url = {http://doi.acm.org/10.1145/1102351.1102441},
	pages = {713--719},
	publisher = {ACM},
	isbn = {1-59593-180-5},
	series = {ICML '05},
	booktitle = {{Proceedings of the 22Nd International Conference on Machine Learning}},
	year = {2005},
	address = {New York, NY, USA},
	doi = {10.1145/1102351.1102441}
}



@article{Taylor.2009,
	author = {Taylor, Matthew E. and Stone, Peter},
	year = {2009},
	title = {{Transfer Learning for Reinforcement Learning Domains: A Survey}},
	url = {http://dl.acm.org/citation.cfm?id=1577069.1755839},
	pages = {1633--1685},
	volume = {10},
	issn = {1532-4435},
	journal = {{J. Mach. Learn. Res.}}
}



@book{Czado.2011,
	author = {Czado, Claudia and Schmidt, Thorsten},
	year = {2011},
	title = {{Mathematische Statistik}},
	address = {Berlin, Heidelberg},
	publisher = {{Springer Berlin Heidelberg}},
	doi = {10.1007/978-3-642-17261-8}
}


@incollection{Prahm.2017,
	abstract = {For decades, researchers have attempted to provide patients with an intuitive method to control upper limb prostheses, enabling them to manipulate multiple degrees of freedom continuously and simultaneously using only simple myoelectric signals. However, such controlling schemes are still highly vulnerable to disturbances in the myoelectric signal, due to electrode shifts, posture changes, sweat, fatigue etc. Recent research has demonstrated that such robustness problems can be alleviated by rapid re-calibration of the prosthesis once a day, using only very small amounts of training data (less than one minute of training time). In this contribution, we propose such a re-calibration scheme for a pattern recognition controller based on transfer learning. In a pilot study with able-bodied subjects we demonstrate that high controller accuracy can be re-obtained after strong electrode shift, even for simultaneous movements in multiple degrees of freedom.},
	author = {Prahm, Cosima and Paassen Benjamin and Schulz Alexander and Hammer Barbara and Aszmann Oskar},
	title = {{Transfer Learning for Rapid Re-calibration of a Myoelectric Prosthesis After Electrode Shift}},
	url = {https://doi.org/10.1007/978-3-319-46669-9_28},
	pages = {153--157},
	publisher = {{Springer International Publishing}},
	isbn = {978-3-319-46669-9},
	editor = {Ib{\'a}{\~n}ez, Jaime and Gonz{\'a}lez-Vargas Jos{\'e} and Azor{\'i}n Jos{\'e} Mar{\'i}a and Akay Metin and Pons Jos{\'e} Luis},
	booktitle = {{Converging Clinical and Engineering Research on Neurorehabilitation II: Proceedings of the 3rd International Conference on NeuroRehabilitation (ICNR2016), October 18-21, 2016, Segovia, Spain}},
	year = {2017},
	address = {Cham},
	doi = {10.1007/978-3-319-46669-9{\textunderscore }28}
}

@proceedings{.2015,
	year = {2015},
	title = {{2015 IEEE International Conference on Information and Automation}}
}


@inproceedings{Wang.2015,
	author = {Wang, J. and Ren, H. and Chen, W. and Zhang, P.},
	title = {{A portable artificial robotic hand controlled by EMG signal using ANN classifier}},
	keywords = {Accuracy;ANN classifier;ANN training;artificial limbs;Artificial Neural Network;artificial neural network training;Artificial neural networks;basic hand movements;electromyography;EMG signal;Feature extraction;handicapped aids;manipulators;medical robotics;medical signal detection;microprocessor;Muscles;myoelectric signal;physically disabled people;portable artificial robotic hand;portable robotic hand;power management;robotic hand;Robots;seven degrees of freedom;signal acquisition;surface electromyography signal;Training},
	pages = {2709--2714},
	booktitle = {{2015 IEEE International Conference on Information and Automation}},
	year = {2015},
	doi = {10.1109/ICInfA.2015.7279744}
}



% This file was created with Citavi 5.6.0.2

@inproceedings{Kumar.2010,
	author = {Kumar, M. Pawan and Packer, Benjamin and Koller, Daphne},
	title = {{Self-paced Learning for Latent Variable Models}},
	url = {http://dl.acm.org/citation.cfm?id=2997189.2997322},
	pages = {1189--1197},
	publisher = {{Curran Associates Inc}},
	series = {NIPS'10},
	booktitle = {{Proceedings of the 23rd International Conference on Neural Information Processing Systems}},
	year = {2010},
	address = {USA}
}



@article{Burlina.2017,
	author = {Burlina, Philippe and Pacheco, Katia D. and Joshi, Neil and Freund, David E. and Bressler, Neil M.},
	year = {2017},
	title = {{Comparing Humans and Deep Learning Performance for Grading AMD}},
	url = {https://doi.org/10.1016/j.compbiomed.2017.01.018},
	keywords = {Age-related macular degeneration;(AMD);(DCNNs);Deep Convolutional Neural Networks;deep learning;Retinal image analysis;transfer learning;Universal features},
	pages = {80--86},
	volume = {82},
	number = {C},
	issn = {0010-4825},
	journal = {{Comput. Biol. Med.}},
	doi = {10.1016/j.compbiomed.2017.01.018}
}


@article{Kourou.2015,
	abstract = {Abstract Cancer has been characterized as a heterogeneous disease consisting of many different subtypes. The early diagnosis and prognosis of a cancer type have become a necessity in cancer research, as it can facilitate the subsequent clinical management of patients. The importance of classifying cancer patients into high or low risk groups has led many research teams, from the biomedical and the bioinformatics field, to study the application of machine learning (ML) methods. Therefore, these techniques have been utilized as an aim to model the progression and treatment of cancerous conditions. In addition, the ability of 5ML6 tools to detect key features from complex datasets reveals their importance. A variety of these techniques, including Artificial Neural Networks (ANNs), Bayesian Networks (BNs), Support Vector Machines (SVMs) and Decision Trees (DTs) have been widely applied in cancer research for the development of predictive models, resulting in effective and accurate decision making. Even though it is evident that the use of 5ML6 methods can improve our understanding of cancer progression, an appropriate level of validation is needed in order for these methods to be considered in the everyday clinical practice. In this work, we present a review of recent 5ML6 approaches employed in the modeling of cancer progression. The predictive models discussed here are based on various supervised 5ML6 techniques as well as on different input features and data samples. Given the growing trend on the application of 5ML6 methods in cancer research, we present here the most recent publications that employ these techniques as an aim to model cancer risk or patient outcomes.},
	author = {Kourou, Konstantina and Exarchos, Themis P. and Exarchos, Konstantinos P. and Karamouzis, Michalis V. and Fotiadis, Dimitrios I.},
	year = {2015},
	title = {{Machine learning applications in cancer prognosis and prediction}},
	url = {http://www.sciencedirect.com/science/article/pii/S2001037014000464},
	keywords = {Cancer recurrence;Cancer survival;Cancer susceptibility;Machine Learning;Predictive models},
	pages = {8--17},
	volume = {13},
	issn = {2001-0370},
	journal = {{Computational and Structural Biotechnology Journal}},
	doi = {10.1016/j.csbj.2014.11.005}
}


% This file was created with Citavi 5.6.0.2

@book{Paluszek.2017,
	author = {Paluszek, Michael and Thomas, Stephanie},
	year = {2017},
	title = {{MATLAB Machine Learning}},
	address = {Berkeley, CA},
	publisher = {Apress},
	isbn = {978-1-4842-2249-2},
	doi = {10.1007/978-1-4842-2250-8}
}



% This file was created with Citavi 5.6.0.2

@proceedings{.2010,
	year = {2010},
	title = {{2010 IEEE International Conference on Computational Intelligence and Computing Research}}
}


@inproceedings{Singh.2010,
	author = {Singh, B. and Singh, H. K.},
	title = {{Web Data Mining research: A survey}},
	keywords = {Data mining;Internet;knowledge acquisition;knowledge extraction;semantic web;web content mining;Web data mining;Web mining;web structure mining;web usage mining;web usages mining;World Wide Web},
	pages = {1--10},
	booktitle = {{2010 IEEE International Conference on Computational Intelligence and Computing Research}},
	year = {2010},
	doi = {10.1109/ICCIC.2010.5705856}
}


@article{Wu.2008,
	abstract = {This paper presents the top 10 data mining algorithms identified by the IEEE International Conference on Data Mining (ICDM) in December 2006: C4.5, k-Means, SVM, Apriori, EM, PageRank, AdaBoost, kNN, Naive Bayes, and CART. These top 10 algorithms are among the most influential data mining algorithms in the research community. With each algorithm, we provide a description of the algorithm, discuss the impact of the algorithm, and review current and further research on the algorithm. These 10 algorithms cover classification, clustering, statistical learning, association analysis, and link mining, which are all among the most important topics in data mining research and development.},
	author = {Wu, Xindong and Kumar Vipin and Ross Quinlan J. and Ghosh Joydeep and Yang Qiang and Motoda Hiroshi and McLachlan Geoffrey J. and Ng Angus and Liu Bing and Yu Philip S. and Zhou Zhi-Hua and Steinbach Michael and Hand David J. and Steinberg Dan},
	year = {2008},
	title = {{Top 10 algorithms in data mining}},
	url = {https://doi.org/10.1007/s10115-007-0114-2},
	pages = {1--37},
	volume = {14},
	number = {1},
	issn = {0219-1377},
	journal = {{Knowledge and Information Systems}},
	doi = {10.1007/s10115-007-0114-2}
}



% This file was created with Citavi 5.6.0.2f

@book{Buchholtz.1982,
	author = {Buchholtz, Christiane},
	year = {1982},
	title = {{Grundlagen der Verhaltensphysiologie}},
	address = {Wiesbaden},
	volume = {53},
	publisher = {{Vieweg+Teubner Verlag}},
	isbn = {9783322887856},
	series = {{Vieweg Studium Grundkurs Biologie}},
	doi = {10.1007/978-3-322-88785-6}
}


% This file was created with Citavi 5.7.0.0

@book{Gartner.2012,
	author = {G{\"a}rtner, Bernd and Matousek, Ji{\v{r}}{\'i}},
	year = {2012},
	title = {{Approximation Algorithms and Semidefinite Programming}},
	address = {Berlin, Heidelberg},
	edition = {2012},
	publisher = {{Springer Berlin Heidelberg}},
	isbn = {978-3-642-22015-9},
	doi = {10.1007/978-3-642-22015-9}
}
























